{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b73954-e799-4aba-9d35-7bc828cec5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: launch_all.py [-h] --config CONFIG [--url URL] [--port PORT]\n",
      "                     [--policy POLICY] [--rollout ROLLOUT] [--log-dir LOG_DIR]\n",
      "                     [-wc] [--num-workers NUM_WORKERS]\n",
      "                     [--worker-idx WORKER_IDX] [--lepton-mode]\n",
      "                     [--lepton-job-name LEPTON_JOB_NAME]\n",
      "                     [--lepton-container-image LEPTON_CONTAINER_IMAGE]\n",
      "                     [--lepton-container-port LEPTON_CONTAINER_PORT]\n",
      "                     [--lepton-resource-shape LEPTON_RESOURCE_SHAPE]\n",
      "                     [--lepton-node-group LEPTON_NODE_GROUP]\n",
      "                     [--lepton-max-failure-retry LEPTON_MAX_FAILURE_RETRY]\n",
      "                     [--lepton-max-job-failure-retry LEPTON_MAX_JOB_FAILURE_RETRY]\n",
      "                     [--lepton-env LEPTON_ENV] [--lepton-secret LEPTON_SECRET]\n",
      "                     [--lepton-mount LEPTON_MOUNT]\n",
      "                     [--lepton-image-pull-secrets LEPTON_IMAGE_PULL_SECRETS]\n",
      "                     [--lepton-intra-job-communication LEPTON_INTRA_JOB_COMMUNICATION]\n",
      "                     [--lepton-privileged]\n",
      "                     [--lepton-ttl-seconds-after-finished LEPTON_TTL_SECONDS_AFTER_FINISHED]\n",
      "                     [--lepton-log-collection LEPTON_LOG_COLLECTION]\n",
      "                     [--lepton-node-id LEPTON_NODE_ID]\n",
      "                     [--lepton-queue-priority LEPTON_QUEUE_PRIORITY]\n",
      "                     [--lepton-visibility LEPTON_VISIBILITY]\n",
      "                     [--lepton-shared-memory-size LEPTON_SHARED_MEMORY_SIZE]\n",
      "                     [--lepton-with-reservation LEPTON_WITH_RESERVATION]\n",
      "\n",
      "Launch multiple processes with GPU assignments\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --config CONFIG       Path to TOML configuration file, which specifies the\n",
      "                        detailed configuration for the whole training process\n",
      "                        including algorithm, model, data, parallelism, etc.\n",
      "  --url URL             URL of the controller for the policy and rollout\n",
      "                        replicas to connect to, consisting of IP and port in\n",
      "                        the format ip:port. If not provided, the controller\n",
      "                        will be launched on the local machine. If provided and\n",
      "                        the IP is the local IP, the controller will be\n",
      "                        launched on the local machine. If provided and the IP\n",
      "                        is not the local IP, the controller will be launched\n",
      "                        on the remote machine.\n",
      "  --port PORT           Port of the controller to connect to, default is 8000.\n",
      "                        This is only used when --url is not provided to launch\n",
      "                        the controller on the local machine.\n",
      "  --policy POLICY       Total number of policy replicas to launch in the whole\n",
      "                        system. If not provided, the number of policy replicas\n",
      "                        will be obtained from TOML configuration file.\n",
      "  --rollout ROLLOUT     Total number of rollout replicas to launch in the\n",
      "                        whole system. If not provided, the number of rollout\n",
      "                        replicas will be obtained from TOML configuration\n",
      "                        file.\n",
      "  --log-dir LOG_DIR     Directory to save logs. If not provided, logs will be\n",
      "                        printed to stdout.\n",
      "  -wc, --weight-sync-check\n",
      "                        Whether to do weight sync correctness check between\n",
      "                        policy and rollout for debugging.\n",
      "  --num-workers NUM_WORKERS\n",
      "                        Number of workers to use for the job, default is 1.\n",
      "                        This is used when multi-node training are used for the\n",
      "                        job.\n",
      "  --worker-idx WORKER_IDX\n",
      "                        Worker index for local execution. In Lepton mode, this\n",
      "                        is ignored as worker indices are automatically\n",
      "                        assigned by Lepton.\n",
      "  --lepton-mode         Enable Lepton mode for remote execution\n",
      "\n",
      "Lepton mode options:\n",
      "  --lepton-job-name LEPTON_JOB_NAME, -n LEPTON_JOB_NAME\n",
      "                        Job name\n",
      "  --lepton-container-image LEPTON_CONTAINER_IMAGE\n",
      "                        Container image for the job\n",
      "  --lepton-container-port LEPTON_CONTAINER_PORT\n",
      "                        Ports to expose for the job, in the format\n",
      "                        portnumber[:protocol]\n",
      "  --lepton-resource-shape LEPTON_RESOURCE_SHAPE\n",
      "                        Resource shape for the pod\n",
      "  --lepton-node-group LEPTON_NODE_GROUP, -ng LEPTON_NODE_GROUP\n",
      "                        Node group for the job\n",
      "  --lepton-max-failure-retry LEPTON_MAX_FAILURE_RETRY\n",
      "                        Maximum number of failures to retry per worker\n",
      "  --lepton-max-job-failure-retry LEPTON_MAX_JOB_FAILURE_RETRY\n",
      "                        Maximum number of failures to retry per whole job\n",
      "  --lepton-env LEPTON_ENV, -e LEPTON_ENV\n",
      "                        Environment variables to pass to the job, in the\n",
      "                        format `NAME=VALUE`\n",
      "  --lepton-secret LEPTON_SECRET, -s LEPTON_SECRET\n",
      "                        Secrets to pass to the job\n",
      "  --lepton-mount LEPTON_MOUNT\n",
      "                        Persistent storage to be mounted to the job\n",
      "  --lepton-image-pull-secrets LEPTON_IMAGE_PULL_SECRETS\n",
      "                        Secrets to use for pulling images\n",
      "  --lepton-intra-job-communication LEPTON_INTRA_JOB_COMMUNICATION\n",
      "                        Enable intra-job communication\n",
      "  --lepton-privileged   Run the job in privileged mode\n",
      "  --lepton-ttl-seconds-after-finished LEPTON_TTL_SECONDS_AFTER_FINISHED\n",
      "                        TTL for finished jobs in seconds\n",
      "  --lepton-log-collection LEPTON_LOG_COLLECTION, -lg LEPTON_LOG_COLLECTION\n",
      "                        Enable or disable log collection\n",
      "  --lepton-node-id LEPTON_NODE_ID, -ni LEPTON_NODE_ID\n",
      "                        Node for the job\n",
      "  --lepton-queue-priority LEPTON_QUEUE_PRIORITY, -qp LEPTON_QUEUE_PRIORITY\n",
      "                        Queue priority for the job\n",
      "  --lepton-visibility LEPTON_VISIBILITY\n",
      "                        Visibility of the job (public/private)\n",
      "  --lepton-shared-memory-size LEPTON_SHARED_MEMORY_SIZE\n",
      "                        Shared memory size in MiB\n",
      "  --lepton-with-reservation LEPTON_WITH_RESERVATION\n",
      "                        Reservation ID for dedicated node groups\n"
     ]
    }
   ],
   "source": [
    "# Example of the options to launch processes.\n",
    "# Ignore all the Lepton mode options currently.\n",
    "# --config --port --policy --rollout are the main useful options for now.\n",
    "# Please see the following help to learn their usage.\n",
    "!python ../launch_all.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49c3a6-4d42-4190-ac8f-29cb2be9d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos] 2025-05-18 07:07:51,100 - cosmos - INFO - Number of policy replicas: 1\n",
      "[cosmos] 2025-05-18 07:07:51,100 - cosmos - INFO - Number of rollout replicas: 1\n",
      "[cosmos] 2025-05-18 07:07:51,191 - cosmos - INFO - Detected 8 GPUs: 0, 1, 2, 3, 4, 5, 6, 7\n",
      "[cosmos] 2025-05-18 07:07:51,192 - cosmos - INFO - Launching process with command: /workspace/cosmos-reason1/tools/launch_controller.sh --config ../../configs/cosmos-reason1/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo.toml --port 8000\n",
      "[cosmos] 2025-05-18 07:07:51,193 - cosmos - INFO - Waiting for controller to be ready at localhost:8000\n",
      "python -m cosmos_rl.dispatcher.run_web_panel --port 8000 --config ../../configs/cosmos-reason1/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo.toml\n",
      "[cosmos] 2025-05-18 07:07:57,393 - cosmos - INFO - Attempting to load configuration from ../../configs/cosmos-reason1/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo.toml\n",
      "[cosmos] 2025-05-18 07:07:57,394 - cosmos - INFO - Using Redis port 12800 from config file.\n",
      "[cosmos] 2025-05-18 07:07:57,395 - cosmos - INFO - Dataset nvidia/Cosmos-Reason1-RL-Dataset is already prepared.\n",
      "[cosmos] 2025-05-18 07:07:57,395 - cosmos - INFO - [Controller] Using reward function ['single_choice', 'format'] for GRPO\n",
      "[cosmos] 2025-05-18 07:07:57,783 - cosmos - INFO - Appending split rl, dataset size = 252\n",
      "[cosmos] 2025-05-18 07:07:57,785 - cosmos - INFO - Final dataset size = 252\n",
      "[Controller] Step:   0%|                              | 0/37800 [00:00<?, ?it/s][cosmos] 2025-05-18 07:07:57,786 - cosmos - INFO - Successfully loaded configuration from ../../configs/cosmos-reason1/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo.toml\n",
      "[cosmos] 2025-05-18 07:07:57,787 - cosmos - INFO - Init redis redis-server /opt/redis_config.conf --dbfilename cosmos_rl_8c0afe42-e65a-46ca-827c-7d4d3fef40ec.rdb --save \"\".\n",
      "[cosmos] 2025-05-18 07:07:59,908 - cosmos - INFO - [Controller] Init redis at 0.0.0.0:12800\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m857367\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "[cosmos] 2025-05-18 07:08:00,196 - cosmos - INFO - Controller is ready at localhost:8000\n",
      "[cosmos] 2025-05-18 07:08:00,197 - cosmos - INFO - Commands to be executed: ['/workspace/cosmos-reason1/tools/launch_replica.sh --type policy --ngpus 2', '/workspace/cosmos-reason1/tools/launch_replica.sh --type rollout --ngpus 2']\n",
      "[cosmos] 2025-05-18 07:08:00,197 - cosmos - INFO - GPU devices to be used: ['0,1', '2,3']\n",
      "[cosmos] 2025-05-18 07:08:00,197 - cosmos - INFO - Control URLs to be used: ['localhost:8000', 'localhost:8000']\n",
      "[cosmos] 2025-05-18 07:08:00,197 - cosmos - INFO - Output files: [None, None]\n",
      "[cosmos] 2025-05-18 07:08:00,197 - cosmos - INFO - Launching process with command: /workspace/cosmos-reason1/tools/launch_replica.sh --type policy --ngpus 2\n",
      "[cosmos] 2025-05-18 07:08:00,198 - cosmos - INFO - Launching process with command: /workspace/cosmos-reason1/tools/launch_replica.sh --type rollout --ngpus 2\n",
      "W0518 07:08:02.048000 857582 torch/distributed/run.py:792] \n",
      "W0518 07:08:02.048000 857582 torch/distributed/run.py:792] *****************************************\n",
      "W0518 07:08:02.048000 857582 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0518 07:08:02.048000 857582 torch/distributed/run.py:792] *****************************************\n",
      "W0518 07:08:02.072000 857588 torch/distributed/run.py:792] \n",
      "W0518 07:08:02.072000 857588 torch/distributed/run.py:792] *****************************************\n",
      "W0518 07:08:02.072000 857588 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0518 07:08:02.072000 857588 torch/distributed/run.py:792] *****************************************\n",
      "[rank0]:INFO 05-18 07:08:07 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "[rank1]:INFO 05-18 07:08:07 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "[rank0]:INFO 05-18 07:08:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "[rank1]:INFO 05-18 07:08:07 [__init__.py:239] Automatically detected platform cuda.\n",
      "[rank1]:INFO 05-18 07:08:09 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "[rank1]:INFO 05-18 07:08:09 [__init__.py:239] Automatically detected platform cuda.\n",
      "[rank0]:INFO 05-18 07:08:09 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "[rank0]:INFO 05-18 07:08:09 [__init__.py:239] Automatically detected platform cuda.\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:11,678 - cosmos - INFO - [Rollout] Loaded rollout configuration: {'parallelism': RolloutParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=-1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), 'gpu_memory_utilization': 0.8, 'enable_chunked_prefill': False, 'max_response_length': 6144, 'n_generation': 8, 'batch_size': 4, 'quantization': 'none', 'seed': 42, 'sampling_config': SamplingConfig(temperature=0.6, top_p=0.95, top_k=50, repetition_penalty=1.05)}\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:11,793 - cosmos - INFO - [Policy] Loaded configuration: {'train': TrainingConfig(train_policy=GrpoConfig(type='grpo', dataset_name='nvidia/Cosmos-Reason1-RL-Dataset', dataset_subset='robovqa', dataset_revision='', dataset_train_split=['rl'], dataset_test_split='test', enable_dataset_preprocess=True, prompt_column_name='qa_pairs', choices_column_name='', response_column_name='', system_prompt='', max_pixels=81920, fps=1, vision_asset_column_name='video', reward_function=['single_choice', 'format'], temperature=0.9, epsilon_low=0.2, epsilon_high=0.2, kl_beta=0.0, mu_iterations=1, mini_batch=1), ckpt=CheckpointConfig(enable_checkpoint=True, save_freq=100, save_mode='async', max_keep=2), resume='False', epoch=150, output_dir='./outputs/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo/20250518070757', timestamp='20250518070757', epsilon=1e-06, optm_name='AdamW', optm_lr=2e-06, optm_impl='fused', optm_weight_decay=0.0, optm_betas=[0.9, 0.95], optm_warmup_steps=20, optm_grad_norm_clip=1.0, async_tp_enabled=False, compile=False, param_dtype='bfloat16', fsdp_reduce_dtype='float32', fsdp_offload=False, fsdp_reshard_after_forward='default', train_batch_per_replica=8, sync_weight_interval=1), 'rollout': RolloutConfig(parallelism=RolloutParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=-1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), gpu_memory_utilization=0.8, enable_chunked_prefill=False, max_response_length=6144, n_generation=8, batch_size=4, quantization='none', seed=42, sampling_config=SamplingConfig(temperature=0.6, top_p=0.95, top_k=50, repetition_penalty=1.05)), 'policy': PolicyConfig(parallelism=ParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), model_name_or_path='nvidia/Cosmos-Reason1-7B', model_max_length=10240, model_gradient_checkpointing=True), 'logging': LoggingConfig(enable_logging=False, project_name='cosmos_rl', experiment_name='cosmos-reason1-rl', report_mfu=False), 'redis': '12800'}\n",
      "[cosmos] 2025-05-18 07:08:12,371 - cosmos - INFO - [Controller] All atoms of Replica 3590018c-0813-460d-9845-163b7ac17ef2 has been set.\n",
      "[cosmos] 2025-05-18 07:08:12,372 - cosmos - INFO - Only one policy replica required, no need build mesh.\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,350 - cosmos - INFO - [Policy] Loaded configuration: {'train': TrainingConfig(train_policy=GrpoConfig(type='grpo', dataset_name='nvidia/Cosmos-Reason1-RL-Dataset', dataset_subset='robovqa', dataset_revision='', dataset_train_split=['rl'], dataset_test_split='test', enable_dataset_preprocess=True, prompt_column_name='qa_pairs', choices_column_name='', response_column_name='', system_prompt='', max_pixels=81920, fps=1, vision_asset_column_name='video', reward_function=['single_choice', 'format'], temperature=0.9, epsilon_low=0.2, epsilon_high=0.2, kl_beta=0.0, mu_iterations=1, mini_batch=1), ckpt=CheckpointConfig(enable_checkpoint=True, save_freq=100, save_mode='async', max_keep=2), resume='False', epoch=150, output_dir='./outputs/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo/20250518070757', timestamp='20250518070757', epsilon=1e-06, optm_name='AdamW', optm_lr=2e-06, optm_impl='fused', optm_weight_decay=0.0, optm_betas=[0.9, 0.95], optm_warmup_steps=20, optm_grad_norm_clip=1.0, async_tp_enabled=False, compile=False, param_dtype='bfloat16', fsdp_reduce_dtype='float32', fsdp_offload=False, fsdp_reshard_after_forward='default', train_batch_per_replica=8, sync_weight_interval=1), 'rollout': RolloutConfig(parallelism=RolloutParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=-1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), gpu_memory_utilization=0.8, enable_chunked_prefill=False, max_response_length=6144, n_generation=8, batch_size=4, quantization='none', seed=42, sampling_config=SamplingConfig(temperature=0.6, top_p=0.95, top_k=50, repetition_penalty=1.05)), 'policy': PolicyConfig(parallelism=ParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), model_name_or_path='nvidia/Cosmos-Reason1-7B', model_max_length=10240, model_gradient_checkpointing=True), 'logging': LoggingConfig(enable_logging=False, project_name='cosmos_rl', experiment_name='cosmos-reason1-rl', report_mfu=False), 'redis': '12800'}\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,360 - cosmos - INFO - Building 1-D device mesh with ['tp'], [2]\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,362 - cosmos - INFO - Starting GRPO training...\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,363 - cosmos - INFO - POLICY Replica started at global rank 0, with replica name: 3590018c-0813-460d-9845-163b7ac17ef2\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:12,360 - cosmos - INFO - Building 1-D device mesh with ['tp'], [2]\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:12,361 - cosmos - INFO - Starting GRPO training...\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:12,364 - cosmos - INFO - POLICY Replica started at global rank 1, with replica name: 3590018c-0813-460d-9845-163b7ac17ef2\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,379 - cosmos - INFO - [Rollout] Loaded rollout configuration: {'parallelism': RolloutParallelismConfig(n_init_replicas=1, tp_size=2, cp_size=1, dp_shard_size=-1, pp_size=1, pp_micro_batch_size=1, dp_replicate_size=1, cp_rotate_method='allgather'), 'gpu_memory_utilization': 0.8, 'enable_chunked_prefill': False, 'max_response_length': 6144, 'n_generation': 8, 'batch_size': 4, 'quantization': 'none', 'seed': 42, 'sampling_config': SamplingConfig(temperature=0.6, top_p=0.95, top_k=50, repetition_penalty=1.05)}\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,374 - cosmos - INFO - POLICY Replica 3590018c-0813-460d-9845-163b7ac17ef2 registered to controller\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:12,644 - cosmos - INFO - Building 1-D device mesh with ['tp'], [2]\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:12,644 - cosmos - INFO - Building 1-D device mesh with ['tp'], [2]\n",
      "[rank0]:Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank1]:Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank1]:Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank0]:Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:13,999 - cosmos - INFO - model path nvidia/Cosmos-Reason1-7B is not a directory. Trying to load from HuggingFace Hub...\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:14,056 - cosmos - INFO - Found safetensors in nvidia/Cosmos-Reason1-7B. Ignoring *pytorch_model* and *consolidated* files.\n",
      "[rank0]:\n",
      "[rank0]:Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "[rank0]:Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 125925.99it/s]\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:14,120 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--nvidia--Cosmos-Reason1-7B/snapshots/c786949ffddd38354133d247b0ae7e637384896c\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:14,121 - cosmos - INFO - Vocabulary size: 152064\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:14,299 - cosmos - INFO - Pipeline is not enabled, skipping\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:14,727 - cosmos - INFO - model path nvidia/Cosmos-Reason1-7B is not a directory. Trying to load from HuggingFace Hub...\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:14,781 - cosmos - INFO - Found safetensors in nvidia/Cosmos-Reason1-7B. Ignoring *pytorch_model* and *consolidated* files.\n",
      "[rank1]:\n",
      "[rank1]:Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "[rank1]:Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 120900.12it/s]\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:14,810 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--nvidia--Cosmos-Reason1-7B/snapshots/c786949ffddd38354133d247b0ae7e637384896c\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:14,811 - cosmos - INFO - Vocabulary size: 152064\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:14,997 - cosmos - INFO - Pipeline is not enabled, skipping\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:15,474 - cosmos - INFO - Applied Tensor Parallelism to the model\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:15,471 - cosmos - INFO - Applied Tensor Parallelism to the model\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:15,473 - cosmos - INFO - Applied activation checkpointing to the model\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:15,473 - cosmos - INFO - Applying FSDP(TP-merged: True) to the visual model\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:15,477 - cosmos - INFO - Applied activation checkpointing to the model\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:15,477 - cosmos - INFO - Applying FSDP(TP-merged: True) to the visual model\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:15,858 - cosmos - INFO - Trainer initialized at local rank 0, with seq_len_multiple: 2\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:15,860 - cosmos - INFO - Trainer initialized at local rank 1, with seq_len_multiple: 2\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:17,861 - cosmos - INFO - [POLICY] Init redis at localhost:12800\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:17,873 - cosmos - INFO - Resume not set. Trying to load from HuggingFace...\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:17,863 - cosmos - INFO - [POLICY] Init redis at localhost:12800\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:17,873 - cosmos - INFO - Resume not set. Trying to load from HuggingFace...\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:17,904 - cosmos - INFO - model path nvidia/Cosmos-Reason1-7B is not a directory. Trying to load from HuggingFace Hub...\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:17,904 - cosmos - INFO - Found safetensors in nvidia/Cosmos-Reason1-7B. Ignoring *pytorch_model* and *consolidated* files.\n",
      "[rank0]:\n",
      "[rank0]:Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "[rank0]:Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 79368.20it/s]\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:17,934 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--nvidia--Cosmos-Reason1-7B/snapshots/c786949ffddd38354133d247b0ae7e637384896c\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:17,907 - cosmos - INFO - model path nvidia/Cosmos-Reason1-7B is not a directory. Trying to load from HuggingFace Hub...\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:17,907 - cosmos - INFO - Found safetensors in nvidia/Cosmos-Reason1-7B. Ignoring *pytorch_model* and *consolidated* files.\n",
      "[rank1]:\n",
      "[rank1]:Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "[rank1]:Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 45935.93it/s]\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:17,936 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--nvidia--Cosmos-Reason1-7B/snapshots/c786949ffddd38354133d247b0ae7e637384896c\n",
      "[rank1]:[cosmos] 2025-05-18 07:08:21,372 - cosmos - INFO - [Policy] Model loaded from checkpoint.\n",
      "[rank0]:[cosmos] 2025-05-18 07:08:21,564 - cosmos - INFO - [Policy] Model loaded from checkpoint.\n",
      "[rank0]:INFO 05-18 07:08:25 [config.py:717] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "[rank0]:INFO 05-18 07:08:25 [config.py:1729] Disabling V1 multiprocessing for external launcher.\n",
      "[rank0]:INFO 05-18 07:08:25 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "[rank1]:INFO 05-18 07:08:25 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "[rank1]:INFO 05-18 07:08:25 [config.py:1729] Disabling V1 multiprocessing for external launcher.\n",
      "[rank1]:INFO 05-18 07:08:25 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "[rank1]:INFO 05-18 07:08:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='nvidia/Cosmos-Reason1-7B', speculative_config=None, tokenizer='nvidia/Cosmos-Reason1-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=10240, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=nvidia/Cosmos-Reason1-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=True, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "[rank1]:WARNING 05-18 07:08:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7303169e9760>\n",
      "[rank0]:INFO 05-18 07:08:26 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='nvidia/Cosmos-Reason1-7B', speculative_config=None, tokenizer='nvidia/Cosmos-Reason1-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=10240, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=nvidia/Cosmos-Reason1-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=True, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "[rank0]:WARNING 05-18 07:08:26 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d05243c9520>\n",
      "[rank0]:INFO 05-18 07:08:26 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "[rank0]:INFO 05-18 07:08:26 [pynccl.py:69] vLLM is using nccl==2.24.3\n",
      "[rank1]:INFO 05-18 07:08:26 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "[rank1]:INFO 05-18 07:08:26 [pynccl.py:69] vLLM is using nccl==2.24.3\n",
      "[rank0]:INFO 05-18 07:08:27 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_03984449'), local_subscribe_addr='ipc:///tmp/b5c33e52-ac45-46b1-915f-581924a6ecba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[rank0]:INFO 05-18 07:08:27 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "[rank0]:INFO 05-18 07:08:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "[rank1]:INFO 05-18 07:08:27 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "[rank1]:INFO 05-18 07:08:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "[rank0]:WARNING 05-18 07:08:31 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[rank0]:INFO 05-18 07:08:31 [gpu_model_runner.py:1329] Starting to load model nvidia/Cosmos-Reason1-7B...\n",
      "[rank0]:INFO 05-18 07:08:31 [config.py:3614] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "[rank0]:INFO 05-18 07:08:31 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "[rank1]:WARNING 05-18 07:08:31 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[rank1]:INFO 05-18 07:08:31 [gpu_model_runner.py:1329] Starting to load model nvidia/Cosmos-Reason1-7B...\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "[rank1]:INFO 05-18 07:08:32 [config.py:3614] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "[rank1]:INFO 05-18 07:08:32 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.75it/s]\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.32it/s]\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.07it/s]\n",
      "[rank0]:\n",
      "[rank0]:Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]\n",
      "[rank0]:\n",
      "[rank0]:INFO 05-18 07:08:35 [loader.py:458] Loading weights took 3.55 seconds\n",
      "[rank0]:INFO 05-18 07:08:35 [gpu_model_runner.py:1347] Model loading took 7.8685 GiB and 4.195534 seconds\n",
      "[rank1]:INFO 05-18 07:08:36 [loader.py:458] Loading weights took 3.89 seconds\n",
      "[rank1]:INFO 05-18 07:08:36 [gpu_model_runner.py:1347] Model loading took 7.8685 GiB and 4.345026 seconds\n",
      "[rank0]:INFO 05-18 07:08:38 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "[rank1]:INFO 05-18 07:08:39 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "[rank0]:INFO 05-18 07:08:51 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1f7cc9e9ce/rank_0_0 for vLLM's torch.compile\n",
      "[rank0]:INFO 05-18 07:08:51 [backends.py:430] Dynamo bytecode transform time: 9.04 s\n",
      "[rank1]:INFO 05-18 07:08:55 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1f7cc9e9ce/rank_1_0 for vLLM's torch.compile\n",
      "[rank1]:INFO 05-18 07:08:55 [backends.py:430] Dynamo bytecode transform time: 12.39 s\n",
      "[rank0]:INFO 05-18 07:08:57 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 5.618 s\n",
      "[rank1]:INFO 05-18 07:09:04 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.221 s\n",
      "[rank0]:INFO 05-18 07:09:09 [monitor.py:33] torch.compile takes 9.04 s in total\n",
      "[rank1]:INFO 05-18 07:09:09 [monitor.py:33] torch.compile takes 12.39 s in total\n",
      "[rank0]:INFO 05-18 07:09:10 [kv_cache_utils.py:634] GPU KV cache size: 1,775,392 tokens\n",
      "[rank0]:INFO 05-18 07:09:10 [kv_cache_utils.py:637] Maximum concurrency for 10,240 tokens per request: 173.38x\n",
      "[rank1]:INFO 05-18 07:09:10 [kv_cache_utils.py:634] GPU KV cache size: 1,775,392 tokens\n",
      "[rank1]:INFO 05-18 07:09:10 [kv_cache_utils.py:637] Maximum concurrency for 10,240 tokens per request: 173.38x\n",
      "[rank0]:INFO 05-18 07:09:44 [gpu_model_runner.py:1686] Graph capturing finished in 33 secs, took 1.00 GiB\n",
      "[rank1]:INFO 05-18 07:09:44 [gpu_model_runner.py:1686] Graph capturing finished in 34 secs, took 1.00 GiB\n",
      "[rank0]:INFO 05-18 07:09:44 [core.py:159] init engine (profile, create kv cache, warmup model) took 68.35 seconds\n",
      "[rank1]:INFO 05-18 07:09:44 [core.py:159] init engine (profile, create kv cache, warmup model) took 67.57 seconds\n",
      "[cosmos] 2025-05-18 07:09:47,157 - cosmos - INFO - [Controller] All atoms of Replica ea9cf958-ae81-4c4a-8ce4-f613f62686b8 has been set.\n",
      "[rank0]:[cosmos] 2025-05-18 07:09:47,152 - cosmos - INFO - ROLLOUT Replica started at global rank 0, with replica name: ea9cf958-ae81-4c4a-8ce4-f613f62686b8\n",
      "[rank0]:[cosmos] 2025-05-18 07:09:47,160 - cosmos - INFO - ROLLOUT Replica ea9cf958-ae81-4c4a-8ce4-f613f62686b8 registered to controller\n",
      "[rank1]:[cosmos] 2025-05-18 07:09:47,152 - cosmos - INFO - ROLLOUT Replica started at global rank 1, with replica name: ea9cf958-ae81-4c4a-8ce4-f613f62686b8\n",
      "[rank0]:[cosmos] 2025-05-18 07:09:49,162 - cosmos - INFO - [ROLLOUT] Init redis at localhost:12800\n",
      "[Controller] Step:   0%|                | 1/37800 [01:51<1169:29:32, 111.38s/it][rank1]:[cosmos] 2025-05-18 07:09:49,162 - cosmos - INFO - [ROLLOUT] Init redis at localhost:12800\n",
      "[rank0]:[cosmos] 2025-05-18 07:09:49,923 - cosmos - INFO - [Policy] Create policy to rollout nccl comm: 0 for 3590018c-0813-460d-9845-163b7ac17ef2_ea9cf958-ae81-4c4a-8ce4-f613f62686b8\n",
      "[rank1]:[cosmos] 2025-05-18 07:09:49,923 - cosmos - INFO - [Policy] Create policy to rollout nccl comm: 0 for 3590018c-0813-460d-9845-163b7ac17ef2_ea9cf958-ae81-4c4a-8ce4-f613f62686b8\n",
      "[rank1]:qwen-vl-utils using torchvision to read video.\n",
      "[rank0]:qwen-vl-utils using torchvision to read video.\n",
      "[rank1]:[2025-05-18 07:11:05] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15153154635070409433.mp4', total_frames=75, video_fps=29.97002997002997, time=0.247s\n",
      "[rank0]:[2025-05-18 07:11:05] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15153154635070409433.mp4', total_frames=75, video_fps=29.97002997002997, time=0.318s\n",
      "[rank1]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5139553518979048294.mp4', total_frames=75, video_fps=29.97002997002997, time=0.179s\n",
      "[rank0]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5139553518979048294.mp4', total_frames=75, video_fps=29.97002997002997, time=0.182s\n",
      "[rank1]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15989326570159403562.mp4', total_frames=100, video_fps=9.997239053495655, time=0.114s\n",
      "[rank0]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15989326570159403562.mp4', total_frames=100, video_fps=9.997239053495655, time=0.114s\n",
      "[rank0]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/9785010101417924213.mp4', total_frames=100, video_fps=9.990003963731853, time=0.163s\n",
      "[rank1]:[2025-05-18 07:11:06] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/9785010101417924213.mp4', total_frames=100, video_fps=9.990003963731853, time=0.150s\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:36,077 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[Controller] Step:   0%|                  | 5/37800 [03:38<412:58:41, 39.34s/it][rank1]:[cosmos] 2025-05-18 07:11:36,077 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank0]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1640564561646893543.mp4', total_frames=95, video_fps=9.504011317312882, time=0.178s\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:36,204 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:11:36,191 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1640564561646893543.mp4', total_frames=95, video_fps=9.504011317312882, time=0.186s\n",
      "[rank0]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/10278340446422916350.mp4', total_frames=100, video_fps=10.002949629405537, time=0.114s\n",
      "[rank1]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/10278340446422916350.mp4', total_frames=100, video_fps=10.002949629405537, time=0.115s\n",
      "[rank0]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12728426564824848928.mp4', total_frames=75, video_fps=29.97002997002997, time=0.190s\n",
      "[rank1]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12728426564824848928.mp4', total_frames=75, video_fps=29.97002997002997, time=0.234s\n",
      "[rank0]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/412538175398019225.mp4', total_frames=100, video_fps=10.027312937707334, time=0.135s\n",
      "[rank1]:[2025-05-18 07:11:36] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/412538175398019225.mp4', total_frames=100, video_fps=10.027312937707334, time=0.150s\n",
      "[rank1]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/11654654140053816607.mp4', total_frames=97, video_fps=9.702137554186251, time=0.712s\n",
      "[rank0]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/11654654140053816607.mp4', total_frames=97, video_fps=9.702137554186251, time=0.715s\n",
      "[rank0]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12045077196668101284.mp4', total_frames=90, video_fps=8.988316400580551, time=0.137s\n",
      "[rank1]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12045077196668101284.mp4', total_frames=90, video_fps=8.988316400580551, time=0.115s\n",
      "[rank0]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/10713823874355217697.mp4', total_frames=75, video_fps=29.97002997002997, time=0.273s\n",
      "[rank1]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/10713823874355217697.mp4', total_frames=75, video_fps=29.97002997002997, time=0.264s\n",
      "[rank0]:[2025-05-18 07:11:46] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15504723463609235307.mp4', total_frames=100, video_fps=10.003552516924726, time=0.161s\n",
      "[rank1]:[2025-05-18 07:11:45] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/15504723463609235307.mp4', total_frames=100, video_fps=10.003552516924726, time=0.142s\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:55,962 - cosmos - INFO - Step: 1, Loss: 0.01562\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:56,474 - cosmos - INFO - [Policy] Train ack sent for global step 1.\n",
      "[rank1]:[cosmos] 2025-05-18 07:11:56,581 - cosmos - INFO - Step: 1, Loss: 0.01562\n",
      "[rank1]:[cosmos] 2025-05-18 07:11:56,589 - cosmos - INFO - [Policy] Train ack sent for global step 1.\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:56,636 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[cosmos] 2025-05-18 07:11:56,657 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank0]:[cosmos] 2025-05-18 07:11:56,736 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:11:56,736 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank0]:[2025-05-18 07:11:58] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5449411762949671225.mp4', total_frames=75, video_fps=29.97002997002997, time=0.287s\n",
      "[rank1]:[2025-05-18 07:11:58] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5449411762949671225.mp4', total_frames=75, video_fps=29.97002997002997, time=0.447s\n",
      "[rank0]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12345653476949823023.mp4', total_frames=75, video_fps=29.97002997002997, time=0.265s\n",
      "[rank0]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8197584166919828849.mp4', total_frames=99, video_fps=9.822812410096452, time=0.144s\n",
      "[rank1]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12345653476949823023.mp4', total_frames=75, video_fps=29.97002997002997, time=0.350s\n",
      "[rank1]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8197584166919828849.mp4', total_frames=99, video_fps=9.822812410096452, time=0.167s\n",
      "[rank0]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/13118343030283267829.mp4', total_frames=100, video_fps=9.99329616354706, time=0.180s\n",
      "[rank1]:[2025-05-18 07:11:59] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/13118343030283267829.mp4', total_frames=100, video_fps=9.99329616354706, time=0.180s\n",
      "[rank0]:[2025-05-18 07:12:12] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/9985322576809247378.mp4', total_frames=75, video_fps=29.97002997002997, time=0.242s\n",
      "[rank1]:[2025-05-18 07:12:12] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/9985322576809247378.mp4', total_frames=75, video_fps=29.97002997002997, time=0.318s\n",
      "[rank0]:[2025-05-18 07:12:12] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/6891423895103306001.mp4', total_frames=75, video_fps=29.97002997002997, time=0.294s\n",
      "[rank1]:[2025-05-18 07:12:13] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/6891423895103306001.mp4', total_frames=75, video_fps=29.97002997002997, time=0.476s\n",
      "[rank0]:[2025-05-18 07:12:13] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/7497939171693343496.mp4', total_frames=96, video_fps=9.622083622083622, time=0.643s\n",
      "[rank0]:[2025-05-18 07:12:13] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/7870899951443367351.mp4', total_frames=100, video_fps=10.001625761184238, time=0.133s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:13,912 - cosmos - INFO - Step: 2, Loss: -0.03125\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:13,918 - cosmos - INFO - Step: 2, Loss: -0.03125\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:13,923 - cosmos - INFO - [Policy] Train ack sent for global step 2.\n",
      "[rank1]:[2025-05-18 07:12:14] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/7497939171693343496.mp4', total_frames=96, video_fps=9.622083622083622, time=0.832s\n",
      "[rank1]:[2025-05-18 07:12:14] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/7870899951443367351.mp4', total_frames=100, video_fps=10.001625761184238, time=0.137s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:14,423 - cosmos - INFO - [Policy] Train ack sent for global step 2.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:15,547 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:15,558 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:15,741 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:15,808 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[2025-05-18 07:12:30] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8867139255103556014.mp4', total_frames=98, video_fps=9.799811151875202, time=0.170s\n",
      "[rank0]:[2025-05-18 07:12:30] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8867139255103556014.mp4', total_frames=98, video_fps=9.799811151875202, time=0.173s\n",
      "[rank1]:[2025-05-18 07:12:30] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/11002986783785622790.mp4', total_frames=94, video_fps=9.408598335187387, time=0.164s\n",
      "[rank0]:[2025-05-18 07:12:30] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/11002986783785622790.mp4', total_frames=94, video_fps=9.408598335187387, time=0.168s\n",
      "[rank1]:[2025-05-18 07:12:31] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12741246367935775703.mp4', total_frames=99, video_fps=9.901144507183858, time=0.165s\n",
      "[rank0]:[2025-05-18 07:12:31] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12741246367935775703.mp4', total_frames=99, video_fps=9.901144507183858, time=0.167s\n",
      "[rank1]:[2025-05-18 07:12:31] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/197674662651845988.mp4', total_frames=100, video_fps=10.011783737048349, time=0.155s\n",
      "[rank0]:[2025-05-18 07:12:31] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/197674662651845988.mp4', total_frames=100, video_fps=10.011783737048349, time=0.155s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:37,125 - cosmos - INFO - Step: 3, Loss: -0.01562\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:37,123 - cosmos - INFO - Step: 3, Loss: -0.01562\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:37,128 - cosmos - INFO - [Policy] Train ack sent for global step 3.\n",
      "[rank1]:[2025-05-18 07:12:37] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/16368527953194494272.mp4', total_frames=97, video_fps=9.70776088213063, time=0.123s\n",
      "[rank0]:[2025-05-18 07:12:37] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/16368527953194494272.mp4', total_frames=97, video_fps=9.70776088213063, time=0.129s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:37,635 - cosmos - INFO - [Policy] Train ack sent for global step 3.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:37,679 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:37,682 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[2025-05-18 07:12:37] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/4245015583345488610.mp4', total_frames=70, video_fps=29.97002997002997, time=0.189s\n",
      "[rank0]:[2025-05-18 07:12:37] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/4245015583345488610.mp4', total_frames=70, video_fps=29.97002997002997, time=0.189s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:37,844 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:37,864 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[2025-05-18 07:12:38] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/13501097194599576722.mp4', total_frames=100, video_fps=10.010289172831204, time=0.146s\n",
      "[rank0]:[2025-05-18 07:12:38] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/13501097194599576722.mp4', total_frames=100, video_fps=10.010289172831204, time=0.147s\n",
      "[rank1]:[2025-05-18 07:12:38] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8612974209954549347.mp4', total_frames=89, video_fps=8.972575974320506, time=0.193s\n",
      "[rank0]:[2025-05-18 07:12:38] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/8612974209954549347.mp4', total_frames=89, video_fps=8.972575974320506, time=0.195s\n",
      "[rank0]:[2025-05-18 07:12:46] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1406878169832949648.mp4', total_frames=98, video_fps=9.706846389382267, time=0.621s\n",
      "[rank1]:[2025-05-18 07:12:46] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1406878169832949648.mp4', total_frames=98, video_fps=9.706846389382267, time=0.656s\n",
      "[rank1]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/6788721445239724168.mp4', total_frames=101, video_fps=10.011676340814425, time=0.130s\n",
      "[rank0]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/6788721445239724168.mp4', total_frames=101, video_fps=10.011676340814425, time=0.134s\n",
      "[rank1]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12100062929795706075.mp4', total_frames=75, video_fps=29.97002997002997, time=0.335s\n",
      "[rank0]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/12100062929795706075.mp4', total_frames=75, video_fps=29.97002997002997, time=0.327s\n",
      "[rank0]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/17074401836255245035.mp4', total_frames=75, video_fps=29.97002997002997, time=0.232s\n",
      "[rank1]:[2025-05-18 07:12:47] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/17074401836255245035.mp4', total_frames=75, video_fps=29.97002997002997, time=0.239s\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:53,557 - cosmos - INFO - Step: 4, Loss: 0.00000\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:53,560 - cosmos - INFO - Step: 4, Loss: 0.00000\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:53,566 - cosmos - INFO - [Policy] Train ack sent for global step 4.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:54,067 - cosmos - INFO - [Policy] Train ack sent for global step 4.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:54,154 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:54,120 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank0]:[cosmos] 2025-05-18 07:12:54,328 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:12:54,286 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[2025-05-18 07:12:56] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/3099823885368784385.mp4', total_frames=100, video_fps=9.9904192806654, time=0.170s\n",
      "[rank0]:[2025-05-18 07:12:56] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/3099823885368784385.mp4', total_frames=100, video_fps=9.9904192806654, time=0.160s\n",
      "[rank1]:[2025-05-18 07:12:56] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1173244175263684373.mp4', total_frames=100, video_fps=9.99521052335713, time=0.169s\n",
      "[rank0]:[2025-05-18 07:12:56] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/1173244175263684373.mp4', total_frames=100, video_fps=9.99521052335713, time=0.175s\n",
      "[rank1]:[2025-05-18 07:12:57] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/17829774109414188560.mp4', total_frames=75, video_fps=29.97002997002997, time=0.245s\n",
      "[rank0]:[2025-05-18 07:12:57] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/17829774109414188560.mp4', total_frames=75, video_fps=29.97002997002997, time=0.241s\n",
      "[rank0]:[2025-05-18 07:12:57] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5733975118425617204.mp4', total_frames=99, video_fps=9.805305562698209, time=0.131s\n",
      "[rank1]:[2025-05-18 07:12:57] INFO vision_process.py:218: torchvision:  video_path='/root/.cache/cosmos/datasets/nvidia/Cosmos-Reason1-RL-Dataset/robovqa/video_clips/clips/5733975118425617204.mp4', total_frames=99, video_fps=9.805305562698209, time=0.150s\n",
      "[rank0]:[cosmos] 2025-05-18 07:13:13,191 - cosmos - INFO - Step: 5, Loss: -0.03125\n",
      "[rank1]:[cosmos] 2025-05-18 07:13:13,194 - cosmos - INFO - Step: 5, Loss: -0.03125\n",
      "[rank1]:[cosmos] 2025-05-18 07:13:13,201 - cosmos - INFO - [Policy] Train ack sent for global step 5.\n",
      "[rank0]:[cosmos] 2025-05-18 07:13:13,702 - cosmos - INFO - [Policy] Train ack sent for global step 5.\n",
      "[rank0]:[cosmos] 2025-05-18 07:13:13,747 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank1]:[cosmos] 2025-05-18 07:13:13,790 - cosmos - INFO - [Policy] Prepare training data.\n",
      "[rank0]:[cosmos] 2025-05-18 07:13:13,940 - cosmos - INFO - [Policy] Start training with prompts 8.\n",
      "[rank1]:[cosmos] 2025-05-18 07:13:13,999 - cosmos - INFO - [Policy] Start training with prompts 8.\n"
     ]
    }
   ],
   "source": [
    "# Example to run the whole grpo processes.\n",
    "# --config to specify the detailed training configuration, here use 2 gpus for each policy node and 2 gpus for each rollout node. \n",
    "# --port 8000 to make the controller web service at the localhost 8000 port.\n",
    "# --policy to specify the total number of policy nodes.\n",
    "# --rollout to specify the total number of rollout nodes.\n",
    "!python ../launch_all.py --config ../../configs/cosmos-reason1/cosmos-reason1-7b-p-fsdp1-tp2-r-tp2-pp1-grpo.toml --port 8000 --policy 1 --rollout 1 ../../tools/dataset/cosmos_grpo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be25d3-6a3a-4ead-969c-2ee34b49cf79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
