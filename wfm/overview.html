

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; cosmos-rl 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Overview" href="../vla/overview.html" />
    <link rel="prev" title="FP8 Quantization" href="../quantization/fp8.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            cosmos-rl
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/single_node_example.html">Single node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/dataflow.html">Dataset &amp; Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/customization.html">Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/hf_models_support.html">Hugging Face Model Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/load_balanced_batching.html">Load-Balanced Dynamic Batching</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Rollout</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rollout/vllm.html">vLLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rollout/trtllm.html">[Experimental] TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi nodes training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/overview.html">Multi-node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/dgxc_lepton.html">DGXC-Lepton Job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/slurm.html">Slurm Job</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Elastic &amp; Fault Tolerance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../elastic/overview.html">Elastic Scaling and Fault Tolerance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Async RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#key-features">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#architecture">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#putting-it-all-together">Putting It All Together</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallelism</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelism/overview.html">Parallelism</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/fp8.html">FP8 Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">World Foundational Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#sft">SFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rl">RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rl-deprecated">RL (deprecated)</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vision-Language-Action Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vla/overview.html">Overview</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">cosmos-rl</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/wfm/overview.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h1>
<p>Cosmos-RL provides native support for SFT and RL of world foundational models.</p>
<section id="sft">
<h2>SFT<a class="headerlink" href="#sft" title="Link to this heading"></a></h2>
<p><strong>Supported Models</strong></p>
<ul class="simple">
<li><p>SD3</p></li>
<li><p>Cosmos-Predict2.5</p></li>
<li><p>SANA-Image/Video</p></li>
</ul>
<p>We construct the WFM SFT training pipeline based on diffusers library. Thus it is easy to extend to other diffusion-based WFMs.</p>
<p>We support both LoRA and full model finetuning for SFT.</p>
<p><strong>Quick start</strong>: A quick start guide for world foundational model’s SFT (Take SD3 as an example):</p>
<ol class="arabic">
<li><p>Configure the training recipe by editing toml files under <code class="docutils literal notranslate"><span class="pre">configs/stable-diffusion-3-5/</span></code>. For LoRA finetuning, you can refer to the example config file <code class="docutils literal notranslate"><span class="pre">stable-diffusion-3-5-image-sft-lora.toml</span></code>. For full model finetuning, you can refer to <code class="docutils literal notranslate"><span class="pre">stable-diffusion-3-5-image-sft.toml</span></code>.</p></li>
<li><p>Configure the dataset and datapacker. The Cosmos-RL is very flexible for the user-customized dataset and input/output format. You can edit the <code class="docutils literal notranslate"><span class="pre">./cosmos_rl/tools/dataset/diffusers_dataset.py</span></code> launcher to customize your own dataset for training. For more details about the customization of dataset, please refer to <a class="reference external" href="https://nvidia-cosmos.github.io/cosmos-rl/quickstart/customization.html">Customization</a>.</p></li>
<li><p>Launch the training script with the configured recipe:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cosmos</span><span class="o">-</span><span class="n">rl</span> <span class="o">--</span><span class="n">config</span> <span class="o">./</span><span class="n">configs</span><span class="o">/</span><span class="n">stable</span><span class="o">-</span><span class="n">diffusion</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="mi">5</span><span class="o">/</span><span class="n">stable</span><span class="o">-</span><span class="n">diffusion</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="mi">5</span><span class="o">-</span><span class="n">image</span><span class="o">-</span><span class="n">sft</span><span class="o">-</span><span class="n">lora</span><span class="o">.</span><span class="n">toml</span> <span class="n">cosmos_rl</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">diffusers_dataset</span>
</pre></div>
</div>
</li>
<li><p>Monitor training progress via Wandb.</p></li>
<li><p>Evaluate the trained world foundational model using the diffusers-based script (coming soon).</p></li>
</ol>
</section>
<section id="rl">
<h2>RL<a class="headerlink" href="#rl" title="Link to this heading"></a></h2>
<p>Cosmos-RL supports <a class="reference external" href="https://arxiv.org/pdf/2509.16117">DiffusionNFT</a> algorithm for world foundational model reinforcement learning.</p>
<p><strong>Supported Models</strong>:</p>
<ul class="simple">
<li><p>SD3</p></li>
<li><p>Cosmos-Predict2.5</p></li>
<li><p>SANA-Image/Video (coming soon)</p></li>
</ul>
<p>We construct the WFM RL training pipeline based on diffusers library. Thus it is easy to extend to other diffusion-based WFMs.</p>
<p><strong>Quick start</strong>: A quick start guide for world foundational model’s RL (Take DiffusionNFT with Cosmos-Predict2.5 as an example):</p>
<ol class="arabic">
<li><p>Install the required dependencies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">.</span><span class="p">[</span><span class="n">wfm</span><span class="p">]</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">cosmos_guardrail</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">deps</span> <span class="c1"># Required by the Cosmos2_5_PredictBasePipeline</span>
</pre></div>
</div>
</li>
<li><p>Configure the training recipe by editing toml files, you can take <code class="docutils literal notranslate"><span class="pre">configs/cosmos-predict2-5/cosmos-predict2-5-2b-720-nft.toml</span></code> as an example.</p></li>
<li><p>Launch the reward service, you can refer docs here: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-rl/tree/main/reward_service">Reward Service</a>.</p></li>
<li><p>Launch the training script with the configured recipe:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cosmos</span><span class="o">-</span><span class="n">rl</span> <span class="o">--</span><span class="n">config</span> <span class="o">./</span><span class="n">configs</span><span class="o">/</span><span class="n">cosmos</span><span class="o">-</span><span class="n">predict2</span><span class="o">-</span><span class="mi">5</span><span class="o">/</span><span class="n">cosmos</span><span class="o">-</span><span class="n">predict2</span><span class="o">-</span><span class="mi">5</span><span class="o">-</span><span class="mi">2</span><span class="n">b</span><span class="o">-</span><span class="mi">720</span><span class="o">-</span><span class="n">nft</span><span class="o">.</span><span class="n">toml</span> <span class="n">cosmos_rl</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">diffusion_nft</span>
</pre></div>
</div>
</li>
<li><p>Monitor training progress via Wandb.</p></li>
<li><p>Evaluate the trained world foundational model using the diffusers-based script (coming soon).</p></li>
</ol>
<p><strong>Reward services</strong>: Considering the computation overhead, it’s necessary to use a seperated async service for reward computing.</p>
<ul>
<li><p>You can launch a reward service by following the instructions here: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-rl/tree/main/reward_service">Reward Service</a>.</p></li>
<li><p>Configure the environment variable <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_TOKEN</span></code>, <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_ENQUEUE_URL</span></code>, and <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_FETCH_URL</span></code> to make the trainer communicate with the reward service:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">REMOTE_REWARD_TOKEN</span><span class="o">=</span><span class="s2">&quot;your_token&quot;</span>
<span class="n">export</span> <span class="n">REMOTE_REWARD_ENQUEUE_URL</span><span class="o">=</span><span class="s2">&quot;https://reward_service_host:PORT/api/reward/enqueue&quot;</span>
<span class="n">export</span> <span class="n">REMOTE_REWARD_FETCH_URL</span><span class="o">=</span><span class="s2">&quot;https://reward_service_host:PORT/api/reward/fetch&quot;</span>
</pre></div>
</div>
</li>
</ul>
<p><strong>Parallelism</strong>: Support FSDP for world foundational model training. You can edit the related configurations in the toml file to enable these parallelism techniques.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">rollout</span><span class="o">.</span><span class="n">parallelism</span><span class="p">]</span>
<span class="n">dp_shard_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="p">[</span><span class="n">policy</span><span class="o">.</span><span class="n">parallelism</span><span class="p">]</span>
<span class="n">dp_shard_size</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use colocated mode for the diffusion-based WFM RL training, i.e., the rollout workers and the policy trainer run on the same set of GPUs. Thus the parallelism settings for both rollout and policy should be the same.</p>
</div>
<p><strong>Datasets</strong>:</p>
<p>The Cosmos-RL is very flexible for the user-customized dataset and input/output format. You can edit the <code class="docutils literal notranslate"><span class="pre">./cosmos_rl/tools/dataset/diffusion_nft.py</span></code> launcher to customize your own dataset for training. For more details about the customization of dataset, please refer to <a class="reference external" href="https://nvidia-cosmos.github.io/cosmos-rl/quickstart/customization.html">Customization</a>.</p>
</section>
<section id="rl-deprecated">
<h2>RL (deprecated)<a class="headerlink" href="#rl-deprecated" title="Link to this heading"></a></h2>
<p>Cosmos-RL supports <a class="reference external" href="https://arxiv.org/pdf/2505.05470">FlowGRPO</a> and <a class="reference external" href="https://arxiv.org/pdf/2512.04332">DDRL</a> algorithms for world foundational model reinforcement learning.</p>
<p><strong>Quick start</strong>: A quick start guide for world foundational model’s RL:</p>
<ol class="arabic">
<li><p>Configure the training recipe by editing toml files under <code class="docutils literal notranslate"><span class="pre">configs/cosmos-predict2-5/</span></code>.</p></li>
<li><p>Launch the reward service, you can refer docs here: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-rl/tree/main/reward_service">Reward Service</a>.</p></li>
<li><p>Launch the training script with the configured recipe:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cosmos</span><span class="o">-</span><span class="n">rl</span> <span class="o">--</span><span class="n">config</span> <span class="o">./</span><span class="n">configs</span><span class="o">/</span><span class="n">cosmos</span><span class="o">-</span><span class="n">predict2</span><span class="o">-</span><span class="mi">5</span><span class="o">/</span><span class="n">cosmos</span><span class="o">-</span><span class="n">predict2</span><span class="o">-</span><span class="mi">5</span><span class="o">-</span><span class="mi">2</span><span class="n">b</span><span class="o">-</span><span class="mi">480</span><span class="o">-</span><span class="n">grpo</span><span class="o">-</span><span class="n">mock</span><span class="o">-</span><span class="n">data</span><span class="o">.</span><span class="n">toml</span> <span class="o">--</span><span class="n">wfm</span><span class="o">-</span><span class="n">mode</span> <span class="n">cosmos_rl</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">wfm_rl</span>
</pre></div>
</div>
</li>
<li><p>Monitor training progress via Wandb.</p></li>
<li><p>Evaluate the trained world foundational model using the evaluation script.
For Cosmos-Predict2.5, you can refer this repo: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-predict2.5">cosmos-predict2.5</a>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>You can find detailed tutorials for DDRL here: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-rl/blob/main/examples/ddrl.md">DDRL Tutorials</a>.</p></li>
<li><p>For a quick rollout of the training pipeline, we recommend you use the mock_data config file, i.e., ./configs/cosmos-predict2-5/cosmos-predict2-5-2b-480-grpo-mock-data.toml</p></li>
</ol>
</div>
<p><strong>Reward services</strong>: Considering the computation overhead, it’s necessary to use a seperated async service for reward computing.</p>
<ul>
<li><p>You can launch a reward service by following the instructions here: <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-rl/tree/main/reward_service">Reward Service</a>.</p></li>
<li><p>Configure the environment variable <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_TOKEN</span></code>, <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_ENQUEUE_URL</span></code>, and <code class="docutils literal notranslate"><span class="pre">REMOTE_REWARD_FETCH_URL</span></code> to make the trainer communicate with the reward service:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">REMOTE_REWARD_TOKEN</span><span class="o">=</span><span class="s2">&quot;your_token&quot;</span>
<span class="n">export</span> <span class="n">REMOTE_REWARD_ENQUEUE_URL</span><span class="o">=</span><span class="s2">&quot;https://reward_service_host:PORT/api/reward/enqueue&quot;</span>
<span class="n">export</span> <span class="n">REMOTE_REWARD_FETCH_URL</span><span class="o">=</span><span class="s2">&quot;https://reward_service_host:PORT/api/reward/fetch&quot;</span>
</pre></div>
</div>
</li>
</ul>
<p><strong>Models</strong>:</p>
<ul class="simple">
<li><p>Cosmos-Predict2.5-2B/14B</p></li>
</ul>
<p><strong>Parallelism</strong>: Support HSDP/FSDP, and context parallel (CP) for world foundational model training. You can edit the related configurations in the toml file to enable these parallelism techniques.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">model</span><span class="p">]</span>
<span class="n">fsdp_shard_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dp_replicate_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="p">[</span><span class="n">model_parallel</span><span class="p">]</span>
<span class="n">context_parallel_size</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p><strong>Datasets</strong>:</p>
<ul>
<li><p>Local dataset: you can use local dataset for training. We follows the local dataset structure as <a class="reference external" href="https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/post-training_video2world_cosmos_nemo_assets.md">Cosmos-Predict2.5</a>. The dataset folder format should be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>datasets/&lt;your_local_dataset&gt;/
├── metas/
│   └── *.txt
├── videos/
│   └── *.mp4
└── text_embedding &lt;optional&gt; /
    └── *.pickle
</pre></div>
</div>
</li>
<li><p>Webdataset: you need to configure the s3 access via environment variables, then you can use webdataset for training.</p>
<blockquote>
<div><ul class="simple">
<li><p>PROD_S3_CHECKPOINT_ACCESS_KEY_ID: Your S3 access key ID.</p></li>
<li><p>PROD_S3_CHECKPOINT_SECRET_ACCESS_KEY: Your S3 secret access key.</p></li>
<li><p>PROD_S3_CHECKPOINT_ENDPOINT_URL: Your S3 endpoint url.</p></li>
<li><p>PROD_S3_CHECKPOINT_REGION_NAME: Your S3 region name.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p><strong>Storage</strong>:</p>
<ul class="simple">
<li><p>Local storage: you can use local disk for storing checkpoints and logs.</p></li>
<li><p>S3 storage: you need to configure the s3 access via environment variables, then you can use s3 storage for storing checkpoints and logs.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../quantization/fp8.html" class="btn btn-neutral float-left" title="FP8 Quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../vla/overview.html" class="btn btn-neutral float-right" title="Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>