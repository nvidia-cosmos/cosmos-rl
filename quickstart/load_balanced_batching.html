

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Load-Balanced Dynamic Batching &mdash; cosmos-rl 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="&lt;no title&gt;" href="../rollout/overview.html" />
    <link rel="prev" title="Hugging Face Model Support" href="hf_models_support.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            cosmos-rl
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="single_node_example.html">Single node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataflow.html">Dataset &amp; Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="customization.html">Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="hf_models_support.html">Hugging Face Model Support</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Load-Balanced Dynamic Batching</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-features">Key Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-it-works">How It Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batch-formation-strategy">Batch Formation Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-parameters">Configuration Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage-example">Usage Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-with-sequence-packing">Example with Sequence Packing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-accumulation">Gradient Accumulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#infinite-loop-and-epoch-management">Infinite Loop and Epoch Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#resume-support">Resume Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-details">Implementation Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#shardediterabledataset">ShardedIterableDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loadbalanceddataset">LoadBalancedDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#best-fit-algorithm">Best-Fit Algorithm</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advantages">Advantages</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#best-practices">Best Practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#when-to-use-sequence-packing">When to Use Sequence Packing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#related-documentation">Related Documentation</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Rollout</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rollout/vllm.html">vLLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rollout/trtllm.html">[Experimental] TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi nodes training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/overview.html">Multi-node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/dgxc_lepton.html">DGXC-Lepton Job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/slurm.html">Slurm Job</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Elastic &amp; Fault Tolerance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../elastic/overview.html">Elastic Scaling and Fault Tolerance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Async RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#key-features">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#architecture">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#putting-it-all-together">Putting It All Together</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallelism</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelism/overview.html">Parallelism</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/fp8.html">FP8 Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">World Foundational Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../wfm/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vision-Language-Action Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vla/overview.html">Overview</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">cosmos-rl</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Load-Balanced Dynamic Batching</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quickstart/load_balanced_batching.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="load-balanced-dynamic-batching">
<h1>Load-Balanced Dynamic Batching<a class="headerlink" href="#load-balanced-dynamic-batching" title="Link to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>Load-balanced dynamic batching is a data loading strategy designed to minimize padding waste and improve training efficiency in distributed training scenarios. Unlike traditional fixed-size batching, this approach dynamically creates batches that maximize token utilization while respecting a maximum token constraint per batch.</p>
</section>
<section id="key-features">
<h2>Key Features<a class="headerlink" href="#key-features" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Dynamic Batch Formation</strong>: Batches are created on-the-fly based on sample lengths, maximizing batch size while staying within token limits</p></li>
<li><p><strong>Load Balancing</strong>: Balances the number of tokens across different data parallel ranks, reducing padding waste</p></li>
<li><p><strong>Step-Based Training</strong>: Training is controlled by optimizer steps (<code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>), not epochs. User-provided epoch configuration is ignored</p></li>
<li><p><strong>Automatic Data Looping</strong>: When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code> (default), data automatically restarts when exhausted, with epoch incremented for new data ordering</p></li>
<li><p><strong>Gradient Accumulation Support</strong>: Built-in support for accumulating multiple batches per optimizer step</p></li>
<li><p><strong>Resume Support</strong>: Properly handles training resumption with deterministic data ordering based on train_step</p></li>
</ul>
</section>
<section id="how-it-works">
<h2>How It Works<a class="headerlink" href="#how-it-works" title="Link to this heading"></a></h2>
<p>The load-balanced batching system consists of two main components:</p>
<ol class="arabic simple">
<li><p><strong>ShardedIterableDataset</strong>: Shards the base dataset across data parallel ranks and converts it to an IterableDataset</p></li>
<li><p><strong>LoadBalancedDataset</strong>: Maintains a pool of samples and dynamically creates batches using a best-fit strategy</p></li>
</ol>
<p><strong>Training Mode</strong>:
- When <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>, training is <strong>step-based</strong>, not epoch-based
- Training duration is controlled by <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code> (number of optimizer steps)
- User-provided <code class="docutils literal notranslate"><span class="pre">epoch</span></code> configuration parameter is <strong>ignored</strong>
- Epoch is managed internally for deterministic data ordering (different epoch = different shuffle)
- When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code> (default), data automatically restarts when exhausted, with epoch incremented
- Each rank may consume data at different rates due to dynamic batching, but training stops when <code class="docutils literal notranslate"><span class="pre">total_steps</span></code> is reached</p>
</section>
<section id="batch-formation-strategy">
<h2>Batch Formation Strategy<a class="headerlink" href="#batch-formation-strategy" title="Link to this heading"></a></h2>
<p>The system uses a pool-based approach:</p>
<ol class="arabic">
<li><p><strong>Sample Pool</strong>: Each rank maintains a pool of samples (default: 32 samples)</p></li>
<li><p><strong>Best-Fit Selection</strong>: When forming a batch, the system selects samples from the pool based on the batching mode:</p>
<p><strong>Without Sequence Packing</strong> (default):
- Maximizes batch_size * max_input_len while staying within <code class="docutils literal notranslate"><span class="pre">max_tokens_for_batch</span></code>
- Uses padding to align sequences to the same length
- Batching strategies:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prefer_closest</span></code>: Selects samples with lengths closest to existing samples in the batch (minimizes padding)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefer_first</span></code>: FIFO selection (faster but may have more padding)</p></li>
</ul>
</div></blockquote>
<p><strong>With Sequence Packing</strong> (<code class="docutils literal notranslate"><span class="pre">sequence_packing</span> <span class="pre">=</span> <span class="pre">true</span></code>):
- Maximizes total tokens (sum of all sequence lengths) while staying within <code class="docutils literal notranslate"><span class="pre">max_tokens_for_batch</span></code>
- Multiple sequences are packed into a single tensor without padding
- Uses a simpler greedy strategy: adds sequences until total tokens exceed the limit
- More efficient token utilization, but requires model support for sequence packing</p>
</li>
</ol>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading"></a></h2>
<p>Load-balanced batching is configured through the training policy configuration:</p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[train.train_policy]</span>
<span class="n">enable_dp_load_balancing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span>
<span class="n">load_balanced_pool_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span>
<span class="n">load_balanced_max_tokens_for_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32768</span>
<span class="n">load_balanced_batching_strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;prefer_closest&quot;</span><span class="w">  </span><span class="c1"># or &quot;prefer_first&quot;</span>
<span class="n">load_balanced_max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span>
<span class="n">load_balanced_batches_per_optimizer_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w">  </span><span class="c1"># Also known as load_balanced_accumulate_steps</span>

<span class="k">[train]</span>
<span class="n">sequence_packing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">false</span><span class="w">  </span><span class="c1"># Set to true to enable sequence packing</span>
</pre></div>
</div>
</section>
<section id="configuration-parameters">
<h2>Configuration Parameters<a class="headerlink" href="#configuration-parameters" title="Link to this heading"></a></h2>
<dl>
<dt>enable_dp_load_balancing</dt><dd><p>Enable load-balanced dynamic batching (default: false)</p>
</dd>
<dt>load_balanced_pool_size</dt><dd><p>Size of the sample pool maintained by each rank (default: 32)</p>
<p>Larger pool sizes allow better batch formation but use more memory.</p>
</dd>
<dt>load_balanced_max_tokens_for_batch</dt><dd><p>Maximum number of tokens per batch (default: 32768)</p>
<p>This is the primary constraint for batch formation. The system will create batches that maximize batch_size * max_input_len while staying within this limit.</p>
</dd>
<dt>load_balanced_batching_strategy</dt><dd><p>Batching strategy: “prefer_closest” or “prefer_first” (default: “prefer_closest”)</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prefer_closest</span></code>: Minimizes padding by selecting samples with similar lengths</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prefer_first</span></code>: FIFO selection, faster but may have more padding</p></li>
</ul>
</dd>
<dt>load_balanced_max_steps</dt><dd><p>Maximum number of optimizer steps (training steps) for load-balanced batching (default: 100)</p>
<p>This defines the number of times <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> will be called. The actual number of batches processed will be <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span> <span class="pre">*</span> <span class="pre">load_balanced_batches_per_optimizer_step</span></code>.</p>
<p><strong>Important</strong>: When <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>, training is <strong>step-based</strong>, not epoch-based. The user-provided <code class="docutils literal notranslate"><span class="pre">epoch</span></code> configuration parameter is ignored. The system uses <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code> to determine when training should stop.</p>
</dd>
<dt>load_balanced_batches_per_optimizer_step</dt><dd><p>Number of batches to accumulate per optimizer step for gradient accumulation (default: 1)</p>
<p>Each DataLoader iteration will return this many batches, which are processed before calling <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>. The total number of batches processed = <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span> <span class="pre">*</span> <span class="pre">load_balanced_batches_per_optimizer_step</span></code>.</p>
</dd>
<dt>sequence_packing</dt><dd><p>Enable sequence packing for training (default: false)</p>
<p>When enabled, multiple sequences are packed into a single tensor without padding, maximizing token utilization. The batch formation strategy changes from maximizing <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">*</span> <span class="pre">max_input_len</span></code> to maximizing <code class="docutils literal notranslate"><span class="pre">sum(sequence_lengths)</span></code> within the token limit.</p>
<p><strong>Important</strong>: Sequence packing requires model support. Not all models support sequence packing. The system will check compatibility and warn if the model doesn’t support it.</p>
<p>When sequence packing is enabled:
- The batching strategy (<code class="docutils literal notranslate"><span class="pre">prefer_closest</span></code> vs <code class="docutils literal notranslate"><span class="pre">prefer_first</span></code>) is ignored
- A greedy algorithm is used: sequences are added until total tokens exceed the limit
- More efficient token utilization compared to padding-based batching
- Requires the model to handle variable-length sequences within a batch</p>
</dd>
</dl>
</section>
<section id="usage-example">
<h2>Usage Example<a class="headerlink" href="#usage-example" title="Link to this heading"></a></h2>
<p>Here’s a complete configuration example:</p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[train.train_policy]</span>
<span class="n">enable_dp_load_balancing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span>
<span class="n">load_balanced_pool_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">64</span>
<span class="n">load_balanced_max_tokens_for_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">65536</span>
<span class="n">load_balanced_batching_strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;prefer_closest&quot;</span>
<span class="n">load_balanced_max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span>
<span class="n">load_balanced_batches_per_optimizer_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">dataloader_seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">42</span>
</pre></div>
</div>
<p>In this example:
- Each rank maintains a pool of 64 samples
- Maximum tokens per batch is 65536
- Uses “prefer_closest” strategy to minimize padding
- Training will run for 1000 optimizer steps
- Each optimizer step accumulates 4 batches (gradient accumulation)
- Total batches processed = 1000 * 4 = 4000 batches</p>
<section id="example-with-sequence-packing">
<h3>Example with Sequence Packing<a class="headerlink" href="#example-with-sequence-packing" title="Link to this heading"></a></h3>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[train]</span>
<span class="n">sequence_packing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span>

<span class="k">[train.train_policy]</span>
<span class="n">enable_dp_load_balancing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span>
<span class="n">load_balanced_pool_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">64</span>
<span class="n">load_balanced_max_tokens_for_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">65536</span>
<span class="n">load_balanced_max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span>
<span class="n">load_balanced_batches_per_optimizer_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">dataloader_seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">42</span>
</pre></div>
</div>
<p>In this example with sequence packing:
- Sequence packing is enabled (no padding needed)
- Maximum total tokens per batch is 65536 (sum of all sequence lengths)
- The batching strategy is automatically set to greedy packing
- More efficient token utilization compared to padding-based batching</p>
</section>
</section>
<section id="gradient-accumulation">
<h2>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Link to this heading"></a></h2>
<p>Load-balanced batching supports gradient accumulation at the data loading level. When <code class="docutils literal notranslate"><span class="pre">load_balanced_batches_per_optimizer_step</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>:</p>
<ol class="arabic simple">
<li><p>Each DataLoader iteration returns a list of batches (instead of a single batch)</p></li>
<li><p>The trainer processes all batches in the list, accumulating gradients</p></li>
<li><p>A single <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> is called after processing all batches</p></li>
</ol>
<p>This approach moves gradient accumulation logic from the trainer to the data loading layer, providing better modularity and efficiency.</p>
</section>
<section id="infinite-loop-and-epoch-management">
<h2>Infinite Loop and Epoch Management<a class="headerlink" href="#infinite-loop-and-epoch-management" title="Link to this heading"></a></h2>
<p>When <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>, the system uses an <code class="docutils literal notranslate"><span class="pre">infinite_loop</span></code> parameter to control data iteration behavior:</p>
<p><strong>Infinite Loop (default: true)</strong>:
- When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code>: Data automatically restarts when exhausted</p>
<blockquote>
<div><ul class="simple">
<li><p>Epoch is automatically incremented each time data restarts</p></li>
<li><p>Different epochs use different random seeds for data shuffling (deterministic but varied ordering)</p></li>
<li><p>This ensures training can reach <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code> even if data is exhausted</p></li>
<li><p>Recommended for step-based training where you want to train for a fixed number of optimizer steps</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">false</span></code>: Data stops when exhausted
- Training stops when all data has been processed
- Not recommended for step-based training as training may stop before reaching <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code></p></li>
</ul>
<p><strong>Epoch Management</strong>:
- Epoch is <strong>managed internally</strong> and is used only for deterministic data ordering
- User-provided <code class="docutils literal notranslate"><span class="pre">epoch</span></code> configuration parameter is <strong>ignored</strong> when <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>
- Epoch does not control training duration (training is step-based, controlled by <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>)
- When data restarts (<code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code>), epoch is automatically incremented to ensure different data ordering
- Initial epoch is set to 0 when resuming training</p>
<p><strong>Why Infinite Loop?</strong>:
- In dynamic batching, different ranks consume data at different rates
- Some ranks may exhaust their data shard before reaching <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>
- With <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code>, data automatically restarts, ensuring all ranks can continue training
- Training stops when <code class="docutils literal notranslate"><span class="pre">train_step</span> <span class="pre">&gt;=</span> <span class="pre">total_steps</span></code> (where <code class="docutils literal notranslate"><span class="pre">total_steps</span> <span class="pre">=</span> <span class="pre">load_balanced_max_steps</span></code>), not when data is exhausted</p>
</section>
<section id="resume-support">
<h2>Resume Support<a class="headerlink" href="#resume-support" title="Link to this heading"></a></h2>
<p>Load-balanced batching properly handles training resumption:</p>
<ol class="arabic simple">
<li><p><strong>Step-Based Training</strong>: Training is based on optimizer steps (<code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>), not epochs</p></li>
<li><p><strong>Automatic Epoch Management</strong>: Epoch is automatically managed internally for deterministic data ordering
- Initial epoch is set to 0 when resuming
- Epoch is automatically incremented when data loops (if <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code>)
- Different epochs use different random seeds for data shuffling</p></li>
<li><p><strong>Batch Skipping</strong>: Skips batch groups that have already been processed based on <code class="docutils literal notranslate"><span class="pre">train_step</span></code></p></li>
</ol>
<p>The resume logic ensures that:
- Data ordering matches the original training (deterministic shuffling)
- Only batches within the current step range are skipped
- Training continues from the correct position</p>
<p><strong>Note</strong>: The user-provided <code class="docutils literal notranslate"><span class="pre">epoch</span></code> configuration parameter is <strong>ignored</strong> when <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>. Epoch is managed internally for data ordering purposes only.</p>
</section>
<section id="implementation-details">
<h2>Implementation Details<a class="headerlink" href="#implementation-details" title="Link to this heading"></a></h2>
<section id="shardediterabledataset">
<h3>ShardedIterableDataset<a class="headerlink" href="#shardediterabledataset" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">ShardedIterableDataset</span></code> class:</p>
<ul class="simple">
<li><p>Shards the base dataset across data parallel ranks</p></li>
<li><p>Converts a regular <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> to an <code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code></p></li>
<li><p>Supports deterministic shuffling based on epoch number</p></li>
<li><p>Ensures each rank only sees its portion of the data</p></li>
</ul>
</section>
<section id="loadbalanceddataset">
<h3>LoadBalancedDataset<a class="headerlink" href="#loadbalanceddataset" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LoadBalancedDataset</span></code> class:</p>
<ul class="simple">
<li><p>Maintains a pool of samples for dynamic batch formation</p></li>
<li><p>Implements best-fit batching strategies (with or without sequence packing)</p></li>
<li><p>Supports gradient accumulation by yielding multiple batches per iteration</p></li>
<li><p>Provides <code class="docutils literal notranslate"><span class="pre">set_epoch()</span></code> and <code class="docutils literal notranslate"><span class="pre">skip_batches()</span></code> methods for resume support</p></li>
<li><p>Supports automatic data looping with <code class="docutils literal notranslate"><span class="pre">infinite_loop</span></code> parameter:
- When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code> (default): Automatically restarts data iteration when exhausted, incrementing epoch for new data ordering
- When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">false</span></code>: Stops iteration when data is exhausted</p></li>
<li><p>Adapts batch formation algorithm based on <code class="docutils literal notranslate"><span class="pre">seq_packing_enabled</span></code> flag:
- Without packing: maximizes batch_size * max_length (with padding)
- With packing: maximizes sum of sequence lengths (without padding)</p></li>
</ul>
<p><strong>Epoch Management</strong>:
- Epoch is managed internally for deterministic data ordering (different epoch = different shuffle)
- When <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code>, epoch is automatically incremented each time data restarts
- Epoch does not control training duration (training is step-based, controlled by <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>)</p>
</section>
<section id="best-fit-algorithm">
<h3>Best-Fit Algorithm<a class="headerlink" href="#best-fit-algorithm" title="Link to this heading"></a></h3>
<p>The best-fit algorithm works differently depending on whether sequence packing is enabled:</p>
<p><strong>Without Sequence Packing</strong> (default):</p>
<ol class="arabic simple">
<li><p>Start with an empty batch</p></li>
<li><p>For each sample in the pool:
- Calculate the new batch size and max length if this sample is added
- Check if the total tokens (batch_size * max_length) &lt;= max_tokens_for_batch
- If valid, calculate a score based on the batching strategy</p></li>
<li><p>Select the sample with the best score (highest for “prefer_closest”, first for “prefer_first”)</p></li>
<li><p>Add the sample to the batch and remove it from the pool</p></li>
<li><p>Repeat until no more samples can be added</p></li>
</ol>
<p><strong>With Sequence Packing</strong> (<code class="docutils literal notranslate"><span class="pre">sequence_packing</span> <span class="pre">=</span> <span class="pre">true</span></code>):</p>
<ol class="arabic simple">
<li><p>Start with an empty batch</p></li>
<li><p>For each sample in the pool (in order):
- Calculate the new total tokens (sum of all sequence lengths) if this sample is added
- Check if the total tokens &lt;= max_tokens_for_batch
- If valid, add the sample to the batch and remove it from the pool</p></li>
<li><p>Repeat until no more samples can be added</p></li>
</ol>
<p>The sequence packing algorithm uses a greedy approach: it adds sequences until the total token count exceeds the limit, maximizing token utilization without padding.</p>
</section>
</section>
<section id="advantages">
<h2>Advantages<a class="headerlink" href="#advantages" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Reduced Padding Waste</strong>: By grouping samples with similar lengths, padding is minimized. With sequence packing enabled, padding is completely eliminated</p></li>
<li><p><strong>Better GPU Utilization</strong>: More tokens per batch means better GPU utilization</p></li>
<li><p><strong>Flexible Batch Sizes</strong>: Adapts to varying sample lengths automatically</p></li>
<li><p><strong>Distributed Training Friendly</strong>: Balances load across ranks while maintaining data distribution</p></li>
<li><p><strong>Sequence Packing Support</strong>: When enabled, eliminates padding entirely by packing multiple sequences into a single tensor, maximizing token efficiency</p></li>
</ol>
</section>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Approximate Length</strong>: <code class="docutils literal notranslate"><span class="pre">len(dataset)</span></code> returns an approximate value based on sample count, not actual batch count</p></li>
<li><p><strong>Memory Overhead</strong>: Maintaining a sample pool requires additional memory</p></li>
<li><p><strong>Step-Based Training</strong>: When <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>, training is step-based, not epoch-based. User-provided epoch configuration is ignored</p></li>
<li><p><strong>Epoch Management</strong>: Epoch is managed internally for data ordering purposes only. It does not control training duration</p></li>
<li><p><strong>Deterministic Resume</strong>: Resume is deterministic based on train_step, but exact batch composition may vary slightly due to dynamic batching</p></li>
</ol>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Pool Size</strong>: Choose a pool size that balances memory usage and batch quality. Larger pools (64-128) work well for most cases</p></li>
<li><p><strong>Max Tokens</strong>: Set <code class="docutils literal notranslate"><span class="pre">max_tokens_for_batch</span></code> based on your GPU memory and model size. Common values: 32768, 65536, 131072</p></li>
<li><p><strong>Batching Strategy</strong>: Use “prefer_closest” for better efficiency, “prefer_first” if speed is more important (only applies when sequence packing is disabled)</p></li>
<li><p><strong>Sequence Packing</strong>: Enable sequence packing if your model supports it for better token utilization. Check model compatibility before enabling</p></li>
<li><p><strong>Gradient Accumulation</strong>: Use <code class="docutils literal notranslate"><span class="pre">load_balanced_batches_per_optimizer_step</span></code> to control effective batch size</p></li>
<li><p><strong>Seed</strong>: Set <code class="docutils literal notranslate"><span class="pre">dataloader_seed</span></code> for reproducibility</p></li>
<li><p><strong>Step-Based Training</strong>: Remember that when <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>, training is step-based. Set <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code> to control training duration, not <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p></li>
<li><p><strong>Infinite Loop</strong>: Keep <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code> (default) for step-based training. Data will automatically restart when exhausted, ensuring training can reach <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code></p></li>
</ol>
<section id="when-to-use-sequence-packing">
<h3>When to Use Sequence Packing<a class="headerlink" href="#when-to-use-sequence-packing" title="Link to this heading"></a></h3>
<p>Sequence packing is recommended when:
- Your model supports variable-length sequences within a batch
- You want to maximize token utilization (reduce padding waste)
- You have sequences with highly variable lengths
- Your model architecture can handle packed sequences efficiently</p>
<p>Sequence packing is NOT recommended when:
- Your model doesn’t support sequence packing (check compatibility)
- You need fixed-size batches for certain operations
- The overhead of handling variable-length sequences outweighs the benefits</p>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h2>
<dl class="simple">
<dt><strong>Issue</strong>: Training seems slower than expected</dt><dd><ul class="simple">
<li><p>Check if <code class="docutils literal notranslate"><span class="pre">load_balanced_pool_size</span></code> is too small</p></li>
<li><p>Verify <code class="docutils literal notranslate"><span class="pre">max_tokens_for_batch</span></code> is appropriate for your hardware</p></li>
<li><p>Consider using “prefer_first” strategy for faster batch formation</p></li>
</ul>
</dd>
<dt><strong>Issue</strong>: Out of memory errors</dt><dd><ul class="simple">
<li><p>Reduce <code class="docutils literal notranslate"><span class="pre">load_balanced_max_tokens_for_batch</span></code></p></li>
<li><p>Reduce <code class="docutils literal notranslate"><span class="pre">load_balanced_pool_size</span></code></p></li>
<li><p>Reduce <code class="docutils literal notranslate"><span class="pre">load_balanced_batches_per_optimizer_step</span></code></p></li>
</ul>
</dd>
<dt><strong>Issue</strong>: Resume doesn’t work correctly</dt><dd><ul class="simple">
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code> matches the original training configuration</p></li>
<li><p>Check that checkpoint contains correct <code class="docutils literal notranslate"><span class="pre">train_step</span></code> information</p></li>
<li><p>Verify <code class="docutils literal notranslate"><span class="pre">dataloader_seed</span></code> is the same as original training</p></li>
<li><p>Note: User-provided <code class="docutils literal notranslate"><span class="pre">epoch</span></code> parameter is ignored when <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code>. Epoch is managed internally</p></li>
</ul>
</dd>
<dt><strong>Issue</strong>: Data keeps looping indefinitely</dt><dd><ul class="simple">
<li><p>This is expected behavior when <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">true</span></code> (default)</p></li>
<li><p>Training stops based on <code class="docutils literal notranslate"><span class="pre">load_balanced_max_steps</span></code>, not data exhaustion</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">infinite_loop</span> <span class="pre">=</span> <span class="pre">false</span></code> if you want data to stop when exhausted (not recommended for step-based training)</p></li>
</ul>
</dd>
<dt><strong>Issue</strong>: Sequence packing not working</dt><dd><ul class="simple">
<li><p>Verify that <code class="docutils literal notranslate"><span class="pre">sequence_packing</span> <span class="pre">=</span> <span class="pre">true</span></code> is set in the configuration</p></li>
<li><p>Check if your model supports sequence packing (the system will warn if not)</p></li>
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">enable_dp_load_balancing</span> <span class="pre">=</span> <span class="pre">true</span></code> is also set</p></li>
<li><p>Check model compatibility: not all models support sequence packing</p></li>
</ul>
</dd>
</dl>
</section>
<section id="related-documentation">
<h2>Related Documentation<a class="headerlink" href="#related-documentation" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="configuration.html"><span class="doc">Configuration</span></a> - General configuration guide</p></li>
<li><p><a class="reference internal" href="dataflow.html"><span class="doc">Dataset &amp; Process</span></a> - Data flow in cosmos-rl</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hf_models_support.html" class="btn btn-neutral float-left" title="Hugging Face Model Support" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../rollout/overview.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>