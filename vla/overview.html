

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; cosmos-rl 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Overview" href="../wfm/overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            cosmos-rl
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/single_node_example.html">Single node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/dataflow.html">Dataset &amp; Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/customization.html">Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/hf_models_support.html">Hugging Face Model Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Rollout</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rollout/vllm.html">vLLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rollout/trtllm.html">[Experimental] TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi nodes training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/overview.html">Multi-node example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/dgxc_lepton.html">DGXC-Lepton Job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multinodes/slurm.html">Slurm Job</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Elastic &amp; Fault Tolerance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../elastic/overview.html">Elastic Scaling and Fault Tolerance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Async RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#key-features">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#architecture">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../async/overview.html#putting-it-all-together">Putting It All Together</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallelism</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../parallelism/overview.html">Parallelism</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/fp8.html">FP8 Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">World Foundational Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../wfm/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vision-Language-Action Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#vla-models">VLA Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#openvla-series">OpenVLA Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pi-series-pi0-5">PI Series (PI0.5)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#simulators">Simulators</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#libero-mujoco-based">LIBERO (MuJoCo-based)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#behavior-1k-isaacsim-based">BEHAVIOR-1K (IsaacSim-based)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#reinforcement-learning-algorithms">Reinforcement Learning Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#grpo-group-relative-policy-optimization">GRPO (Group Relative Policy Optimization)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">cosmos-rl</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/vla/overview.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h1>
<p>Cosmos-RL provides native support for Vision-Language-Action (VLA) model reinforcement learning, enabling embodied AI agents to learn robotic manipulation tasks through interaction with simulators.</p>
<section id="vla-models">
<h2>VLA Models<a class="headerlink" href="#vla-models" title="Link to this heading"></a></h2>
<p>Cosmos-RL supports two major series of VLA models:</p>
<section id="openvla-series">
<h3>OpenVLA Series<a class="headerlink" href="#openvla-series" title="Link to this heading"></a></h3>
<p>OpenVLA is a vision-language-action model built on the Prismatic framework. Cosmos-RL supports two variants:</p>
<ul class="simple">
<li><p><strong>OpenVLA</strong>: The original OpenVLA model architecture</p></li>
<li><p><strong>OpenVLA-OFT</strong>: OpenVLA with Online Fine-Tuning support, optimized for RL training</p></li>
</ul>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p>Based on SigLIP/Dinov2 + LLaMA architecture</p></li>
<li><p>Action prediction through language model token generation</p></li>
<li><p>Norm statistics for action normalization</p></li>
<li><p>Compatible with HuggingFace model hub</p></li>
</ul>
<p><strong>Model Configuration:</strong></p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[policy]</span>
<span class="n">model_name_or_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Haozhan72/Openvla-oft-SFT-libero10-trajall&quot;</span>

<span class="k">[vla]</span>
<span class="n">vla_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;openvla-oft&quot;</span>
<span class="n">training_chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span>
</pre></div>
</div>
</section>
<section id="pi-series-pi0-5">
<h3>PI Series (PI0.5)<a class="headerlink" href="#pi-series-pi0-5" title="Link to this heading"></a></h3>
<p>PI0.5 is a diffusion-based VLA model that uses flow-based action prediction for robotic manipulation.</p>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p>Based on PaliGemma model with Diffusion</p></li>
<li><p>Flow-based action generation with configurable denoising steps</p></li>
<li><p>Expert network for action prediction</p></li>
<li><p>Supports multiple noise methods: flow_sde, flow_cps, flow_noise</p></li>
</ul>
<p><strong>Model Configuration:</strong></p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[policy]</span>
<span class="n">model_name_or_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;sunshk/pi05_libero_pytorch&quot;</span>

<span class="k">[vla]</span>
<span class="n">vla_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;pi05&quot;</span>
<span class="n">training_chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span>

<span class="k">[custom.pi05]</span>
<span class="n">num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span><span class="w">  </span><span class="c1"># denoise steps</span>
<span class="n">action_chunk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span>
<span class="n">action_env_dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">7</span>
<span class="n">noise_method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;flow_sde&quot;</span>
<span class="n">train_expert_only</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span>
</pre></div>
</div>
</section>
</section>
<section id="simulators">
<h2>Simulators<a class="headerlink" href="#simulators" title="Link to this heading"></a></h2>
<p>Cosmos-RL integrates with multiple robotics simulators for VLA training and evaluation:</p>
<section id="libero-mujoco-based">
<h3>LIBERO (MuJoCo-based)<a class="headerlink" href="#libero-mujoco-based" title="Link to this heading"></a></h3>
<p>LIBERO (Lifelong roBotic lEarning benchmaRk with lOng-horizon tasks) is a MuJoCo-based simulation environment for long-horizon manipulation tasks.</p>
<p><strong>Supported Task Suites:</strong></p>
<ul class="simple">
<li><p>libero_spatial: 10 spatial reasoning tasks</p></li>
<li><p>libero_object: 10 object interaction tasks</p></li>
<li><p>libero_goal: 10 goal-oriented tasks</p></li>
<li><p>libero_10: 10 long-horizon manipulation tasks</p></li>
<li><p>libero_90: 90 diverse manipulation tasks</p></li>
<li><p>libero_all: 130 tasks from all suites</p></li>
</ul>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>CPU/GPU rendering support</p></li>
<li><p>Multi-environment parallel rollout</p></li>
<li><p>Task initialization from dataset states</p></li>
<li><p>Each environment can run different tasks simultaneously</p></li>
<li><p>Action space: 7-DoF (position, rotation, gripper)</p></li>
</ul>
<p><strong>Configuration:</strong></p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[validation]</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;libero&quot;</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;libero_10&quot;</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;val&quot;</span>

<span class="k">[vla]</span>
<span class="n">num_envs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">8</span><span class="w">  </span><span class="c1"># parallel environments per rank</span>
</pre></div>
</div>
<p><strong>Data Source:</strong> <a class="reference external" href="https://github.com/Lifelong-Robot-Learning/LIBERO">LIBERO GitHub</a></p>
</section>
<section id="behavior-1k-isaacsim-based">
<h3>BEHAVIOR-1K (IsaacSim-based)<a class="headerlink" href="#behavior-1k-isaacsim-based" title="Link to this heading"></a></h3>
<p>BEHAVIOR-1K is a large-scale benchmark built on OmniGibson/IsaacSim for household manipulation tasks.</p>
<p><strong>Features:</strong></p>
<ul class="simple">
<li><p>GPU-accelerated physics simulation</p></li>
<li><p>1000+ diverse household tasks</p></li>
<li><p>Photorealistic rendering</p></li>
<li><p>Task descriptions from BEHAVIOR benchmark</p></li>
<li><p>Action space in B1K Neurips’25 challenge: 23-DoF</p></li>
</ul>
<p><strong>Configuration:</strong></p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[validation]</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;b1k&quot;</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;b1k&quot;</span>

<span class="k">[vla]</span>
<span class="n">num_envs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span>
<span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span>
</pre></div>
</div>
<p><strong>Requirements:</strong></p>
<ul class="simple">
<li><p>NVIDIA GPU with RT Cores (L20, L40, or RTX series)</p></li>
<li><p>OmniGibson installation</p></li>
<li><p>BEHAVIOR-1K dataset</p></li>
</ul>
<p><strong>Data Source:</strong> Available through OmniGibson</p>
</section>
</section>
<section id="reinforcement-learning-algorithms">
<h2>Reinforcement Learning Algorithms<a class="headerlink" href="#reinforcement-learning-algorithms" title="Link to this heading"></a></h2>
<p>Cosmos-RL supports multiple RL algorithms optimized for VLA training:</p>
<section id="grpo-group-relative-policy-optimization">
<h3>GRPO (Group Relative Policy Optimization)<a class="headerlink" href="#grpo-group-relative-policy-optimization" title="Link to this heading"></a></h3>
<p>GRPO is a policy gradient algorithm designed for efficient on-policy learning without requiring a critic network.</p>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p>No value function / critic network needed</p></li>
<li><p>Group-based advantage estimation using relative rewards</p></li>
<li><p>Efficient for VLA training with sparse rewards</p></li>
<li><p>Supports reward filtering to remove outliers</p></li>
</ul>
<p><strong>Algorithm Configuration:</strong></p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[train.train_policy]</span>
<span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;grpo&quot;</span>
<span class="n">trainer_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;grpo_vla&quot;</span><span class="w">  </span><span class="c1"># or &quot;grpo_pi05&quot; for PI models</span>
<span class="n">variant</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;dapo&quot;</span>
<span class="n">temperature</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.6</span>
<span class="n">epsilon_low</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.2</span>
<span class="n">epsilon_high</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.28</span>
<span class="n">lower_bound_ratio</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">10.0</span>
<span class="n">kl_beta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
</pre></div>
</div>
<p><strong>How It Works:</strong></p>
<ol class="arabic simple">
<li><p>Generate multiple rollouts per task</p></li>
<li><p>Compute relative advantages within each group (task)</p></li>
<li><p>Use policy gradient with clipped objective</p></li>
<li><p>Update policy based on advantage signals</p></li>
<li><p>Change to dapo variant if need asymmetric clipping or dynamic sampling</p></li>
</ol>
</section>
</section>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading"></a></h2>
<p><strong>1. Configure the training recipe</strong> by editing toml files under <code class="docutils literal notranslate"><span class="pre">configs/openvla-oft/</span></code> or <code class="docutils literal notranslate"><span class="pre">configs/pi05/</span></code>.</p>
<p><strong>2. Prepare the simulator environment:</strong></p>
<p>For LIBERO:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/your/workspace/cosmos-rl
uv<span class="w"> </span>sync<span class="w"> </span>--extra<span class="w"> </span>vla
<span class="c1"># or</span>
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="o">[</span>vla<span class="o">]</span>

<span class="nv">ROBOT_PLATFORM</span><span class="o">=</span>LIBERO<span class="w"> </span><span class="se">\</span>
uv<span class="w"> </span>run<span class="w"> </span>cosmos-rl<span class="w"> </span>--config<span class="w"> </span>configs/openvla-oft/openvla-oft-7b-fsdp2-8p8r-colocate.toml<span class="w"> </span><span class="se">\</span>
<span class="w">       </span>--log-dir<span class="w"> </span>logs<span class="w"> </span><span class="se">\</span>
<span class="w">       </span>cosmos_rl/tools/dataset/libero_grpo.py
</pre></div>
</div>
<p>For BEHAVIOR-1K:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/your/workspace/cosmos-rl
apt<span class="w"> </span>install<span class="w"> </span>python3-dev<span class="w"> </span><span class="c1"># if necessary</span>
uv<span class="w"> </span>sync
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate

<span class="nb">cd</span><span class="w"> </span>/your/workspace
git<span class="w"> </span>clone<span class="w"> </span>-b<span class="w"> </span>v3.7.2<span class="w"> </span>https://github.com/StanfordVL/BEHAVIOR-1K.git
<span class="nb">cd</span><span class="w"> </span>BEHAVIOR-1K
uv<span class="w"> </span>add<span class="w"> </span>pip<span class="w"> </span><span class="nv">cffi</span><span class="o">==</span><span class="m">1</span>.17.1<span class="w"> </span><span class="c1"># if necessary</span>
apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>libsm6<span class="w"> </span>libxt6<span class="w"> </span>libglu1-mesa
<span class="nv">UV_LINK_MODE</span><span class="o">=</span>hardlink<span class="w"> </span>./setup.sh<span class="w"> </span>--omnigibson<span class="w"> </span>--bddl<span class="w"> </span>--joylo<span class="w"> </span>--confirm-no-conda<span class="w"> </span>--accept-nvidia-eula
</pre></div>
</div>
<p><strong>3. Prepare the pretrained model:</strong></p>
<p>For PI0.5:</p>
<ul class="simple">
<li><p>We prepared PI0.5 models ranked 2nd in the Neurips’25 BEHAVIOR-1K challenge, converted to PyTorch format</p></li>
<li><p>Download from <a class="reference external" href="https://huggingface.co/fwd4xl/pi05-b1k-pt12-cs32-v1">fwd4xl/pi05-b1k-pt12-cs32-v1</a></p></li>
</ul>
<p><strong>4. Launch training:</strong></p>
<p>For OpenVLA-OFT:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">ROBOT_PLATFORM</span><span class="o">=</span>LIBERO<span class="w"> </span><span class="se">\</span>
uv<span class="w"> </span>run<span class="w"> </span>cosmos-rl<span class="w"> </span>--config<span class="w"> </span>configs/openvla-oft/openvla-oft-7b-fsdp2-8p8r-colocate.toml<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>cosmos_rl/tools/dataset/libero_grpo.py
</pre></div>
</div>
<p>For PI0.5:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>run<span class="w"> </span>cosmos-rl<span class="w"> </span>--config<span class="w"> </span>configs/pi05/pi05-b1k-grpo-colocate.toml<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>cosmos_rl/tools/dataset/b1k_grpo.py
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2406.09246">OpenVLA Paper</a></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2504.16054">PI05</a></p></li>
<li><p><a class="reference external" href="https://github.com/Lifelong-Robot-Learning/LIBERO">LIBERO Benchmark</a></p></li>
<li><p><a class="reference external" href="https://behavior.stanford.edu/challenge/index.html">BEHAVIOR Benchmark</a></p></li>
<li><p><a class="reference external" href="https://github.com/bytedance/SimpleVLA-RL">SimpleVLA-RL</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.03300">GRPO Algorithm</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../wfm/overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>