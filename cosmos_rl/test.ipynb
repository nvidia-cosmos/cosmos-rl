{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84555eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 22:22:10 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-23 22:22:10 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:22:12,232\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from cosmos_rl.policy.model.qwen2_5_vl import Qwen2_5_VLConditionalModel\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82b1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 40.44it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "hf_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f207ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cosmos] 2025-07-23 22:22:28,954 - cosmos - INFO - model path Qwen/Qwen2.5-VL-7B-Instruct is not a directory. Trying to load from HuggingFace Hub...\n",
      "[cosmos] 2025-07-23 22:22:29,242 - cosmos - INFO - Found safetensors in Qwen/Qwen2.5-VL-7B-Instruct. Ignoring *pytorch_model* and *consolidated* files.\n",
      "Fetching 1 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "[cosmos] 2025-07-23 22:22:29,439 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json\n",
      "[cosmos] 2025-07-23 22:22:29,440 - cosmos - INFO - model path Qwen/Qwen2.5-VL-7B-Instruct is not a directory. Trying to load from HuggingFace Hub...\n",
      "[cosmos] 2025-07-23 22:22:29,440 - cosmos - INFO - Found safetensors in Qwen/Qwen2.5-VL-7B-Instruct. Ignoring *pytorch_model* and *consolidated* files.\n",
      "Fetching 1 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "[cosmos] 2025-07-23 22:22:29,593 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model-00005-of-00005.safetensors\n",
      "[cosmos] 2025-07-23 22:22:29,655 - cosmos - INFO - Vocabulary size: 152064\n",
      "[cosmos] 2025-07-23 22:22:29,656 - cosmos - INFO - WeightMapper: QwenVL25WeightMapper is being initialized.\n"
     ]
    }
   ],
   "source": [
    "cosmos_model = Qwen2_5_VLConditionalModel.from_pretrained(\n",
    "    hf_model.config,\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5841fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_model = cosmos_model.to(device).to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11a1cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cosmos] 2025-07-23 22:23:01,184 - cosmos - INFO - model path Qwen/Qwen2.5-VL-7B-Instruct is not a directory. Trying to load from HuggingFace Hub...\n",
      "[cosmos] 2025-07-23 22:23:01,185 - cosmos - INFO - Found safetensors in Qwen/Qwen2.5-VL-7B-Instruct. Ignoring *pytorch_model* and *consolidated* files.\n",
      "Fetching 16 files: 100%|██████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 157532.54it/s]\n",
      "[cosmos] 2025-07-23 22:23:01,339 - cosmos - INFO - Downloaded model from HuggingFace to /root/.cache/huggingface/transformers/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5\n"
     ]
    }
   ],
   "source": [
    "from cosmos_rl.policy.config import ParallelismConfig\n",
    "from cosmos_rl.utils.parallelism import ParallelDims\n",
    "\n",
    "cosmos_model.load_hf_weights(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    parallel_dims=ParallelDims.from_config(\n",
    "        ParallelismConfig(\n",
    "            tp_size=1,\n",
    "            pp_size=1,\n",
    "            dp_replicate_size=1,\n",
    "            dp_shard_size=1,\n",
    "        )\n",
    "    ),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "cosmos_model.model.rotary_emb.inv_freq = cosmos_model.model.rotary_emb.inv_freq.to(torch.float32)\n",
    "cosmos_model.model.rotary_emb.reset_inv_freq()\n",
    "cosmos_model.visual.rotary_pos_emb.inv_freq = cosmos_model.visual.rotary_pos_emb.inv_freq.to(torch.float32)\n",
    "cosmos_model.visual.rotary_pos_emb.reset_inv_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a9d762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "                \"max_pixels\": 89100,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "input_ids = inputs.input_ids\n",
    "image_grid_thw = inputs.image_grid_thw\n",
    "pixel_values_lengths_per_sample = torch.tensor(inputs.pixel_values.shape[0]).to(device).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19805f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7734, -0.1328,  0.2949,  ..., -0.9688, -0.1348, -1.0156],\n",
      "        [-1.0547,  0.1562, -0.7422,  ..., -0.2441, -0.1006, -1.1406],\n",
      "        [-0.1016,  1.3906,  0.3730,  ..., -1.1016,  0.1699, -0.9727],\n",
      "        ...,\n",
      "        [-1.2578,  1.2656,  0.7695,  ...,  1.4062, -1.0000, -2.0000],\n",
      "        [-0.6055,  0.3086,  0.6641,  ..., -1.3438, -0.8984, -0.3516],\n",
      "        [-0.2490,  0.0044, -0.1475,  ..., -0.1826,  0.0996, -0.2373]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "cosmos_model.post_to_empty_hook(None)\n",
    "position_ids, input_ids, seq_dim_idx = cosmos_model.get_position_ids(\n",
    "    **inputs\n",
    ")\n",
    "with torch.no_grad():\n",
    "    cosmos_result = cosmos_model.visual.forward(\n",
    "        inputs.pixel_values.unsqueeze(0).to(torch.bfloat16).to(device),\n",
    "        grid_thw=image_grid_thw.to(device),\n",
    "    )\n",
    "    print(cosmos_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f22586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.6953e-01, -1.2695e-01,  2.8906e-01,  ..., -9.7656e-01,\n",
      "         -1.3477e-01, -1.0078e+00],\n",
      "        [-1.0625e+00,  1.7188e-01, -7.4219e-01,  ..., -2.4609e-01,\n",
      "         -1.1865e-01, -1.1484e+00],\n",
      "        [-1.1182e-01,  1.3906e+00,  3.7500e-01,  ..., -1.0938e+00,\n",
      "          1.7480e-01, -9.7266e-01],\n",
      "        ...,\n",
      "        [-1.2734e+00,  1.2578e+00,  7.6953e-01,  ...,  1.3750e+00,\n",
      "         -1.0156e+00, -2.0156e+00],\n",
      "        [-6.3281e-01,  2.9492e-01,  6.6797e-01,  ..., -1.3516e+00,\n",
      "         -8.7109e-01, -3.5352e-01],\n",
      "        [-2.5195e-01, -1.1292e-03, -1.4648e-01,  ..., -1.8359e-01,\n",
      "          9.6680e-02, -2.4414e-01]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_result = hf_model.visual(\n",
    "        inputs.pixel_values.unsqueeze(0).unsqueeze(0),\n",
    "        inputs.image_grid_thw.to(device),\n",
    "    )\n",
    "    print(hf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729d07c",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a5274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_model.post_to_empty_hook(None)\n",
    "position_ids, input_ids, seq_dim_idx = cosmos_model.get_position_ids(\n",
    "    **inputs\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    cosmos_result = cosmos_model.forward(\n",
    "        input_ids=input_ids,\n",
    "        pixel_values=inputs.pixel_values.unsqueeze(0).to(torch.bfloat16).to(device),\n",
    "        image_grid_thw=image_grid_thw.to(device),\n",
    "        position_ids=position_ids.to(device),\n",
    "        pixel_values_lengths_per_sample=pixel_values_lengths_per_sample.view(-1, 1).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5421b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hf_result = hf_model.forward(\n",
    "        **inputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dadce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.1250, 14.8750, 11.4375,  ...,  1.2500,  1.2500,  1.2500],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_result[-1, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd39ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.1875, 14.8750, 11.4375,  ...,  1.1875,  1.1875,  1.1875],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result.logits[-1, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadc702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.1875, 14.8750, 11.4375,  ...,  1.1875,  1.1875,  1.1875],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_result.logits[-1, -1, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
