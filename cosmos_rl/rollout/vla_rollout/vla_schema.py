"""
VLA-specific data schemas for Cosmos-RL.

This module defines clean interfaces between VLA rollout results and Cosmos-RL's RLPayload system.
"""

from typing import Dict, Any, Optional
from pydantic import BaseModel, Field


class VLAEpisodeMetadata(BaseModel):
    """
    Metadata for a single VLA episode rollout.
    
    This structure bridges SimpleVLA-RL's episode data with Cosmos-RL's RLPayload system.
    """
    
    # Episode outcome (matches SimpleVLA-RL's 'complete' field)
    success: bool = Field(
        description="Whether the task was completed successfully"
    )
    
    # Episode length (matches SimpleVLA-RL's 'finish_step' field)
    episode_length: int = Field(
        description="Number of steps taken in the episode"
    )
    
    # Task information
    task_suite: str = Field(
        description="Task suite name (e.g., 'libero_10', 'robotwin')"
    )
    task_id: int = Field(
        default=0,
        description="Task ID within the suite"
    )
    trial_id: int = Field(
        default=0,
        description="Trial ID for this task"
    )
    
    # Model generation info
    num_response_tokens: int = Field(
        default=0,
        description="Number of action tokens generated by the VLA model"
    )
    
    # Optional: Additional metrics
    total_reward: float = Field(
        default=0.0,
        description="Total reward accumulated during the episode (if available)"
    )
    
    # Prompt information
    prompt_id: int = Field(
        default=0,
        description="Index of the prompt in the batch"
    )
    
    # Generation config
    temperature: float = Field(
        default=1.0,
        description="Sampling temperature used for generation"
    )
    n_generation: int = Field(
        default=1,
        description="Number of generations per prompt"
    )


def compute_vla_reward(metadata: VLAEpisodeMetadata) -> float:
    """
    Compute reward for a VLA episode.
    
    Uses simple binary reward: 1.0 for success, 0.0 for failure.
    
    TODO: Implement more sophisticated reward shaping:
    - Partial credit for progress (e.g., step_reward = success / episode_length)
    - Penalty for overly long episodes
    - Task-specific reward functions
    
    Args:
        metadata: VLA episode metadata
        
    Returns:
        Reward value (0.0 or 1.0)
    """
    # Simple binary reward: 1.0 for success, 0.0 for failure
    return 1.0 if metadata.success else 0.0


def compute_success_rate(rewards: list[float]) -> float:
    """
    Compute success rate from a list of binary rewards.
    
    Args:
        rewards: List of rewards (0.0 or 1.0)
        
    Returns:
        Success rate as a percentage (0.0 to 100.0)
    """
    if not rewards:
        return 0.0
    return sum(rewards) / len(rewards) * 100.0


def create_vla_metadata_from_environment_info(
    environment_info: Dict[str, Any],
    prompt_idx: int = 0,
    temperature: float = 1.0,
    n_generation: int = 1,
) -> VLAEpisodeMetadata:
    """
    Create VLAEpisodeMetadata from environment_info dict.
    
    This is a convenience function to convert the raw environment_info
    from RolloutResult into structured metadata.
    
    Args:
        environment_info: Raw environment info from RolloutResult
        prompt_idx: Index of the prompt
        temperature: Sampling temperature
        n_generation: Number of generations
        
    Returns:
        Structured VLA episode metadata
    """
    return VLAEpisodeMetadata(
        success=environment_info.get('success', False),
        episode_length=environment_info.get('episode_length', 0),
        task_suite=environment_info.get('task_suite', ''),
        task_id=environment_info.get('task_id', 0),
        trial_id=environment_info.get('trial_id', 0),
        num_response_tokens=environment_info.get('num_response_tokens', 0),
        total_reward=environment_info.get('total_reward', 0.0),
        prompt_id=prompt_idx,
        temperature=temperature,
        n_generation=n_generation,
    )

